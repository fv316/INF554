{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/var/python/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/jet/var/python/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/jet/var/python/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/jet/var/python/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/jet/var/python/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/jet/var/python/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/jet/var/python/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/jet/var/python/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/jet/var/python/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/jet/var/python/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/jet/var/python/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/jet/var/python/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/jet/var/python/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "## Packages \n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "#from tqdm import tqdm # progess bar\n",
    "import pickle\n",
    "\n",
    "# For graphs\n",
    "import networkx as nx \n",
    "import stellargraph as sg\n",
    "from stellargraph.data import EdgeSplitter\n",
    "from stellargraph.mapper import GraphSAGELinkGenerator\n",
    "from stellargraph.layer import GraphSAGE, HinSAGE, link_classification\n",
    "from stellargraph import globalvar\n",
    "\n",
    "# For DL\n",
    "from tensorflow import keras \n",
    "######\n",
    "import tensorflow as tf\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=8, inter_op_parallelism_threads=2, allow_soft_placement=True, device_count = {'CPU': 8})\n",
    "\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"8\"\n",
    "\n",
    "os.environ[\"KMP_BLOCKTIME\"] = \"30\"\n",
    "\n",
    "os.environ[\"KMP_SETTINGS\"] = \"1\"\n",
    "\n",
    "os.environ[\"KMP_AFFINITY\"]= \"granularity=fine,verbose,compact,1,0\"\n",
    "\n",
    "#####\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# For processing node texts\n",
    "#import spacy\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.feature_extraction import text as fe\n",
    "\n",
    "# Dimensionality reduction\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "# Word embeddings\n",
    "#import gensim \n",
    "#from gensim.models import Word2Vec\n",
    "\n",
    "# For stemming\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.gpu_device_name()\n",
    "#!cat /proc/meminfo\n",
    "!cat /proc/cpuinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load corpus and ids from pickles\n",
    "corpus_path = r\"pickles/corpus.PICKLE\" \n",
    "ids_path = r\"pickles/IDs.PICKLE\"\n",
    "with open(corpus_path, 'rb') as f:\n",
    "    corpus = pickle.load(f)\n",
    "f.close()\n",
    "with open(ids_path, 'rb') as f:\n",
    "    ids = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# Save in dataframe\n",
    "node_info = pd.DataFrame({'ID': ids, 'Corpus': corpus})\n",
    "node_info_ID = node_info.set_index(['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each set of tokens calculate ratio of each language present\n",
    "def calculate_languages_ratios_from_tokens(tokens):\n",
    "    languages_ratios = []\n",
    "    \n",
    "    # Lower words in set of tokens\n",
    "    words = [word.lower() for word in tokens]\n",
    "    \n",
    "    # Supported languages as intersection\n",
    "    supported_languages = set(stopwords.fileids()) & set(SnowballStemmer.languages)\n",
    "    \n",
    "    # For each language, identify ratio in set of tokens\n",
    "    for language in supported_languages:\n",
    "        stopwords_set = set(stopwords.words(language))\n",
    "        words_set = set(words)\n",
    "        common_elements = words_set.intersection(stopwords_set)\n",
    "        languages_ratios.append(len(common_elements))\n",
    "        \n",
    "    # Set to zero if ratio is zero\n",
    "    if sum(languages_ratios) == 0:\n",
    "        return np.zeros(len(languages_ratios))\n",
    "    \n",
    "    return np.array(languages_ratios)/sum(languages_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stem corpus based on frequent languages\n",
    "\n",
    "# Defining path\n",
    "stemmed_corpus_path = r\"pickles/stemmed_corpus.PICKLE\"\n",
    "\n",
    "\n",
    "if os.path.exists(stemmed_corpus_path):\n",
    "    pass\n",
    "else:\n",
    "    # Supported languages as intersection\n",
    "    supported_languages = list(set(stopwords.fileids()) & set(SnowballStemmer.languages))\n",
    "    stemmed_corpus = []\n",
    "\n",
    "    # For each text in corpus\n",
    "    for text in tqdm(node_info['Corpus'].values, position=0, leave=True):\n",
    "\n",
    "        # Identify tokens\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # Compute language ratios\n",
    "        ratio = calculate_languages_ratios_from_tokens(tokens)\n",
    "\n",
    "        # Note most frequent languages in langs\n",
    "        if np.sum(ratio == 0):\n",
    "            pass\n",
    "        if np.any(ratio>=0.25):\n",
    "            indices = np.where(ratio >= 0.25)[0]\n",
    "            langs = [supported_languages[j] for j in indices]\n",
    "        elif np.all(ratio<0.25) and np.any(ratio>0.10):\n",
    "            indices = np.where(ratio > 0.10)[0]\n",
    "            langs = [supported_languages[j] for j in indices]\n",
    "        else:\n",
    "            langs = [supported_languages[np.argmax(ratio)]]\n",
    "\n",
    "        # For each frequent language stem word if not a stopword \n",
    "        # and if it consists of alphabet letters\n",
    "        for lang in langs:\n",
    "            lang_stopwords = stopwords.words(lang)\n",
    "            stemmer = SnowballStemmer(lang)\n",
    "            tokens = [stemmer.stem(word) for word in tokens if (word not in lang_stopwords) and word.isalpha()]\n",
    "        stemmed_corpus.append(' '.join(tokens))\n",
    "\n",
    "    # Dump pickle\n",
    "    with open(stemmed_corpus_path, '+wb') as f:\n",
    "        pickle.dump(stemmed_corpus, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generating smaller dictionary with stemmed words, no stopwords and frequency > 20\n",
    "\n",
    "# Defining paths\n",
    "small_matrix_path = r\"pickles/small_word_matrix.PICKLE\"\n",
    "corpus_path = r\"pickles/stemmed_corpus.PICKLE\"\n",
    "\n",
    "if os.path.exists(small_matrix_path):\n",
    "    with open(small_matrix_path, 'rb') as f:\n",
    "        word_matrix = pickle.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    with open(corpus_path, 'rb') as f:\n",
    "        stemmed_corpus = pickle.load(f)\n",
    "        \n",
    "        # Get vectorizer from feature extraction package\n",
    "        vectorizer = fe.CountVectorizer(min_df = 20, max_df = 0.9, strip_accents = 'unicode')\n",
    "        \n",
    "        # Vectorize corpus\n",
    "        word_matrix = vectorizer.fit_transform(tqdm(stemmed_corpus))\n",
    "        \n",
    "        # Dump pickle\n",
    "        with open(small_matrix_path, '+wb') as g:\n",
    "            pickle.dump(word_matrix, g)\n",
    "        g.close()\n",
    "    f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_matrix_path = r\"pickles/corpus_tfidf_matrix.PICKLE\"\n",
    "if os.path.exists(corpus_matrix_path):\n",
    "    with open(corpus_matrix_path, 'rb') as f:\n",
    "        corpus_tfidf_matrix = pickle.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    with open(corpus_path, 'rb') as g:\n",
    "        stemmed_corpus = pickle.load(g)\n",
    "        vectorizer3 = fe.TfidfVectorizer(min_df = 20, max_df = 0.9, strip_accents = 'unicode')\n",
    "        corpus_tfidf_matrix = vectorizer3.fit_transform(tqdm(stemmed_corpus))\n",
    "        with open(corpus_matrix_path, '+wb') as f:\n",
    "            pickle.dump(corpus_tfidf_matrix, f)\n",
    "        f.close()\n",
    "    g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = corpus_tfidf_matrix.shape[1]\n",
    "n_components = 100\n",
    "\n",
    "nmf_frobenius_path = r\"pickles/nmf_frobenius_matrix.PICKLE\"\n",
    "if os.path.exists(nmf_frobenius_path):\n",
    "    with open(nmf_frobenius_path, 'rb') as f:\n",
    "        nmf_frobenius = pickle.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
    "          \"n_samples=%d and n_features=%d...\" % (corpus_tfidf_matrix.shape[0], n_features))\n",
    "\n",
    "    nmf_frobenius = NMF(n_components=n_components, random_state=1, alpha=.1, l1_ratio=.5).fit(corpus_tfidf_matrix)\n",
    "    with open(nmf_frobenius_path, '+wb') as f:\n",
    "        pickle.dump(nmf_frobenius, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_reduced_matrix_path = r\"pickles/nmf_frobenius_matrix_realized.PICKLE\"\n",
    "if os.path.exists(small_reduced_matrix_path):\n",
    "    with open(small_reduced_matrix_path, 'rb') as f:\n",
    "        small_reduced_matrix = pickle.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    with open(small_reduced_matrix_path, '+wb') as f:\n",
    "        small_reduced_matrix = nmf_frobenius.fit_transform(corpus_tfidf_matrix)\n",
    "        pickle.dump(small_reduced_matrix, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_path = r\"pickles/lda_matrix.PICKLE\"\n",
    "\n",
    "\n",
    "if os.path.exists(lda_path):\n",
    "    with open(lda_path, 'rb') as f:\n",
    "        lda_matrix = pickle.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    n_features = word_matrix.shape[1]\n",
    "    n_components = 100\n",
    "    print(\"Fitting LDA models with tf features, \"\n",
    "          \"n_samples=%d and n_features=%d...\"\n",
    "          % (word_matrix.shape[0], n_features))\n",
    "    lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                    learning_method='online',\n",
    "                                    learning_offset=50.,\n",
    "                                    random_state=0)\n",
    "    lda.fit(word_matrix)\n",
    "    lda_matrix = lda.fit_transform(word_matrix)\n",
    "    with open(lda_path, '+wb') as f:\n",
    "        pickle.dump(lda_matrix, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33226 500\n"
     ]
    }
   ],
   "source": [
    "# Storing in dataframe\n",
    "n1, n2 = lda_matrix.shape\n",
    "# Creating feature names\n",
    "feature_names = [\"w_{}\".format(ii) for ii in range(n2)]\n",
    "ids = sorted(range(n1), key=str)\n",
    "node_data = pd.DataFrame(data=lda_matrix, index=ids, columns=feature_names)\n",
    "print(n1,n2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               w_0           w_1           w_2           w_3           w_4  \\\n",
      "0     5.797101e-06  5.797101e-06  5.797101e-06  5.797101e-06  5.797101e-06   \n",
      "1     9.319664e-07  9.319664e-07  4.437605e-02  9.319664e-07  9.319664e-07   \n",
      "10    1.323627e-06  1.323627e-06  1.816354e-06  1.323627e-06  1.323627e-06   \n",
      "100   1.316553e-04  2.465207e-02  1.498062e-02  1.635072e-03  1.238620e-07   \n",
      "1000  6.006006e-06  6.006006e-06  6.104773e-06  6.006006e-06  6.006006e-06   \n",
      "...            ...           ...           ...           ...           ...   \n",
      "9995  9.950249e-06  9.950249e-06  9.950249e-06  9.950249e-06  9.950249e-06   \n",
      "9996  1.071237e-06  1.656704e-03  1.071237e-06  1.474441e-03  1.071237e-06   \n",
      "9997  6.009615e-07  6.009615e-07  6.009615e-07  1.773803e-03  6.009615e-07   \n",
      "9998  1.164144e-06  1.164144e-06  5.824716e-02  1.164144e-06  1.164144e-06   \n",
      "9999  1.052632e-04  1.052632e-04  1.052632e-04  1.052632e-04  1.052632e-04   \n",
      "\n",
      "               w_5           w_6           w_7           w_8           w_9  \\\n",
      "0     5.797101e-06  5.797101e-06  5.797101e-06  5.797101e-06  5.797101e-06   \n",
      "1     9.319664e-07  9.319664e-07  9.319664e-07  9.319664e-07  9.319664e-07   \n",
      "10    2.081750e-03  1.323627e-06  1.323627e-06  1.323627e-06  1.323627e-06   \n",
      "100   1.238620e-07  1.238620e-07  6.960537e-03  1.059784e-03  1.238620e-07   \n",
      "1000  6.006006e-06  6.006006e-06  6.006006e-06  6.006006e-06  6.006006e-06   \n",
      "...            ...           ...           ...           ...           ...   \n",
      "9995  9.950249e-06  6.975995e-03  9.950249e-06  9.950249e-06  9.950249e-06   \n",
      "9996  1.071237e-06  1.071237e-06  1.071237e-06  1.071237e-06  1.071237e-06   \n",
      "9997  2.019785e-03  6.009615e-07  6.009615e-07  2.507444e-02  6.009615e-07   \n",
      "9998  1.164144e-06  1.164144e-06  1.164144e-06  3.131756e-03  1.164144e-06   \n",
      "9999  1.052632e-04  1.052632e-04  1.052632e-04  1.052632e-04  1.052632e-04   \n",
      "\n",
      "      ...         w_490         w_491         w_492         w_493  \\\n",
      "0     ...  5.797101e-06  5.797101e-06  5.797101e-06  5.797101e-06   \n",
      "1     ...  9.319664e-07  9.319664e-07  3.584458e-03  9.319664e-07   \n",
      "10    ...  1.323627e-06  1.323627e-06  1.323627e-06  1.323627e-06   \n",
      "100   ...  1.238620e-07  1.238620e-07  1.238620e-07  2.341244e-03   \n",
      "1000  ...  6.006006e-06  6.006006e-06  6.006006e-06  6.006006e-06   \n",
      "...   ...           ...           ...           ...           ...   \n",
      "9995  ...  9.950249e-06  9.950249e-06  9.950249e-06  9.950249e-06   \n",
      "9996  ...  4.603355e-02  1.071237e-06  1.071237e-06  1.071237e-06   \n",
      "9997  ...  6.009615e-07  6.009615e-07  6.009615e-07  6.009615e-07   \n",
      "9998  ...  1.164144e-06  1.164144e-06  1.381569e-03  1.164144e-06   \n",
      "9999  ...  1.052632e-04  1.052632e-04  1.052632e-04  1.052632e-04   \n",
      "\n",
      "             w_494         w_495         w_496         w_497         w_498  \\\n",
      "0     5.797101e-06  5.797101e-06  5.797101e-06  5.797101e-06  5.797101e-06   \n",
      "1     9.319664e-07  9.319664e-07  9.319664e-07  9.319664e-07  9.319664e-07   \n",
      "10    1.323627e-06  4.322455e-02  1.323627e-06  1.323627e-06  1.323627e-06   \n",
      "100   1.238620e-07  4.404484e-03  1.238620e-07  1.238620e-07  1.238620e-07   \n",
      "1000  6.006006e-06  6.006006e-06  6.006006e-06  6.006006e-06  6.006006e-06   \n",
      "...            ...           ...           ...           ...           ...   \n",
      "9995  9.950249e-06  9.950249e-06  9.950249e-06  9.950249e-06  9.950249e-06   \n",
      "9996  1.071237e-06  1.071237e-06  1.071237e-06  1.071237e-06  1.071237e-06   \n",
      "9997  6.009615e-07  6.009615e-07  6.009615e-07  6.009615e-07  6.009615e-07   \n",
      "9998  1.164144e-06  1.164144e-06  1.164144e-06  1.164144e-06  1.164144e-06   \n",
      "9999  1.052632e-04  1.052632e-04  1.052632e-04  1.052632e-04  1.052632e-04   \n",
      "\n",
      "             w_499  \n",
      "0     5.797101e-06  \n",
      "1     9.319664e-07  \n",
      "10    1.323627e-06  \n",
      "100   1.238620e-07  \n",
      "1000  6.006006e-06  \n",
      "...            ...  \n",
      "9995  9.950249e-06  \n",
      "9996  1.071237e-06  \n",
      "9997  6.009615e-07  \n",
      "9998  1.164144e-06  \n",
      "9999  1.052632e-04  \n",
      "\n",
      "[33226 rows x 500 columns]\n"
     ]
    }
   ],
   "source": [
    "print(node_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make graphs great again\n",
    "\n",
    "Time to get training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples shape: (453797, 3)\n",
      "Testing examples shape: (113450, 2)\n"
     ]
    }
   ],
   "source": [
    "# Read training\n",
    "with open(r\"training.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    training  = list(reader)\n",
    "# in order of training examples\n",
    "training = [element[0].split(\" \") for element in training]\n",
    "training = pd.DataFrame(training, columns=['Node1', 'Node2', 'Link'])\n",
    "print(\"Training examples shape: {}\".format(training.shape))\n",
    "\n",
    "# Read testing\n",
    "with open(r\"testing.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    testing  = list(reader)\n",
    "# in order of testing examples\n",
    "testing = [element[0].split(\" \") for element in testing]\n",
    "testing = pd.DataFrame(testing, columns=['Node1', 'Node2'])\n",
    "print(\"Testing examples shape: {}\".format(testing.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_nodes = training.loc[training['Link']=='1']\n",
    "linked_nodes = linked_nodes[['Node1', 'Node2']]\n",
    "linked_nodes.to_csv('linked_nodes.txt', sep=' ', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 1000 edges\n",
      "Removed 2000 edges\n",
      "Removed 3000 edges\n",
      "Removed 4000 edges\n",
      "Removed 5000 edges\n",
      "Removed 6000 edges\n",
      "Removed 7000 edges\n",
      "Removed 8000 edges\n",
      "Removed 9000 edges\n",
      "Removed 10000 edges\n",
      "Removed 11000 edges\n",
      "Removed 12000 edges\n",
      "Removed 13000 edges\n",
      "Removed 14000 edges\n",
      "Removed 15000 edges\n",
      "Removed 16000 edges\n",
      "Removed 17000 edges\n",
      "Removed 18000 edges\n",
      "Removed 19000 edges\n",
      "Removed 20000 edges\n",
      "Removed 21000 edges\n",
      "Removed 22000 edges\n",
      "Removed 23000 edges\n",
      "Removed 24000 edges\n",
      "Removed 25000 edges\n",
      "Removed 26000 edges\n",
      "Removed 27000 edges\n",
      "Removed 28000 edges\n",
      "Sampled 1000 negative examples\n",
      "Sampled 2000 negative examples\n",
      "Sampled 3000 negative examples\n",
      "Sampled 4000 negative examples\n",
      "Sampled 5000 negative examples\n",
      "Sampled 6000 negative examples\n",
      "Sampled 7000 negative examples\n",
      "Sampled 8000 negative examples\n",
      "Sampled 9000 negative examples\n",
      "Sampled 10000 negative examples\n",
      "Sampled 11000 negative examples\n",
      "Sampled 12000 negative examples\n",
      "Sampled 13000 negative examples\n",
      "Sampled 14000 negative examples\n",
      "Sampled 15000 negative examples\n",
      "Sampled 16000 negative examples\n",
      "Sampled 17000 negative examples\n",
      "Sampled 18000 negative examples\n",
      "Sampled 19000 negative examples\n",
      "Sampled 20000 negative examples\n",
      "Sampled 21000 negative examples\n",
      "Sampled 22000 negative examples\n",
      "Sampled 23000 negative examples\n",
      "Sampled 24000 negative examples\n",
      "Sampled 25000 negative examples\n",
      "Sampled 26000 negative examples\n",
      "Sampled 27000 negative examples\n",
      "Sampled 28000 negative examples\n",
      "** Sampled 28362 positive and 28362 negative edges. **\n",
      "Removed 1000 edges\n",
      "Removed 2000 edges\n",
      "Removed 3000 edges\n",
      "Removed 4000 edges\n",
      "Removed 5000 edges\n",
      "Removed 6000 edges\n",
      "Removed 7000 edges\n",
      "Removed 8000 edges\n",
      "Removed 9000 edges\n",
      "Removed 10000 edges\n",
      "Removed 11000 edges\n",
      "Removed 12000 edges\n",
      "Removed 13000 edges\n",
      "Removed 14000 edges\n",
      "Removed 15000 edges\n",
      "Removed 16000 edges\n",
      "Removed 17000 edges\n",
      "Removed 18000 edges\n",
      "Removed 19000 edges\n",
      "Removed 20000 edges\n",
      "Removed 21000 edges\n",
      "Removed 22000 edges\n",
      "Removed 23000 edges\n",
      "Removed 24000 edges\n",
      "Removed 25000 edges\n",
      "Sampled 1000 negative examples\n",
      "Sampled 2000 negative examples\n",
      "Sampled 3000 negative examples\n",
      "Sampled 4000 negative examples\n",
      "Sampled 5000 negative examples\n",
      "Sampled 6000 negative examples\n",
      "Sampled 7000 negative examples\n",
      "Sampled 8000 negative examples\n",
      "Sampled 9000 negative examples\n",
      "Sampled 10000 negative examples\n",
      "Sampled 11000 negative examples\n",
      "Sampled 12000 negative examples\n",
      "Sampled 13000 negative examples\n",
      "Sampled 14000 negative examples\n",
      "Sampled 15000 negative examples\n",
      "Sampled 16000 negative examples\n",
      "Sampled 17000 negative examples\n",
      "Sampled 18000 negative examples\n",
      "Sampled 19000 negative examples\n",
      "Sampled 20000 negative examples\n",
      "Sampled 21000 negative examples\n",
      "Sampled 22000 negative examples\n",
      "Sampled 23000 negative examples\n",
      "Sampled 24000 negative examples\n",
      "Sampled 25000 negative examples\n",
      "** Sampled 25526 positive and 25526 negative edges. **\n"
     ]
    }
   ],
   "source": [
    "# Read edges and create NetworkX graph\n",
    "edgelist = pd.read_csv(\"linked_nodes.txt\", sep=' ', header=None, names=[\"source\", \"target\"])\n",
    "edgelist[\"label\"] = \"cites\"  # set the edge type\n",
    "G_all_nx = nx.from_pandas_edgelist(edgelist, edge_attr=\"label\")\n",
    "G_all_nx.add_nodes_from(ids)\n",
    "nx.set_node_attributes(G_all_nx, \"paper\", \"label\")\n",
    "\n",
    "# Initialize Stellargraph with node features of text\n",
    "#G_all = sg.StellarGraph(G_all_nx, node_features=node_data[feature_names])\n",
    "\n",
    "# Define an edge splitter on the original graph G:\n",
    "edge_splitter_test = EdgeSplitter(G_all_nx)\n",
    "\n",
    "# Randomly sample a fraction p=0.1 of all positive links, \n",
    "# and same number of negative links, from G, and obtain the\n",
    "# reduced graph G_test with the sampled links removed:\n",
    "G_test, edge_ids_test, edge_labels_test = edge_splitter_test.train_test_split(\n",
    "    p=0.1, method=\"global\", keep_connected=True)\n",
    "\n",
    "# Define an edge splitter on the reduced graph G_test:\n",
    "edge_splitter_train = EdgeSplitter(G_test)\n",
    "\n",
    "# Randomly sample a fraction p=0.1 of all positive links, and same number of negative links, from G_test, and obtain the\n",
    "# reduced graph G_train with the sampled links removed:\n",
    "G_train, edge_ids_train, edge_labels_train = edge_splitter_train.train_test_split(\n",
    "    p=0.1, method=\"global\", keep_connected=True)\n",
    "\n",
    "G_test = sg.StellarGraph(G_test, node_features=node_data[feature_names])\n",
    "G_train = sg.StellarGraph(G_train, node_features=node_data[feature_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StellarGraph: Undirected multigraph\n",
      " Nodes: 33226, Edges: 255261\n",
      "\n",
      " Node types:\n",
      "  paper: [33226]\n",
      "    Edge types: paper-cites->paper\n",
      "\n",
      " Edge types:\n",
      "    paper-cites->paper: [255261]\n",
      "\n",
      "StellarGraph: Undirected multigraph\n",
      " Nodes: 33226, Edges: 229735\n",
      "\n",
      " Node types:\n",
      "  paper: [33226]\n",
      "    Edge types: paper-cites->paper\n",
      "\n",
      " Edge types:\n",
      "    paper-cites->paper: [229735]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(G_test.info())\n",
    "print(G_train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /jet/var/python/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "batch_size = 200\n",
    "epochs = 100\n",
    "num_samples = [30, 20]\n",
    "\n",
    "train_gen = GraphSAGELinkGenerator(G_train, batch_size, num_samples).flow(\n",
    "    edge_ids_train, edge_labels_train, shuffle=True)\n",
    "test_gen = GraphSAGELinkGenerator(G_test,  batch_size, num_samples).flow(\n",
    "    edge_ids_test, edge_labels_test)\n",
    "\n",
    "layer_sizes = [20, 20]\n",
    "assert len(layer_sizes) == len(num_samples)\n",
    "\n",
    "graphsage = GraphSAGE(\n",
    "        layer_sizes=layer_sizes, generator=train_gen, bias=True, dropout=0.3)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "link_classification: using 'ip' method to combine node embeddings into edge embeddings\n"
     ]
    }
   ],
   "source": [
    "# Build the model and expose input and output sockets of graphsage model for link prediction via graphsage.build() method\n",
    "x_inp, x_out = graphsage.build()\n",
    "\n",
    "prediction = link_classification(\n",
    "    output_dim=1, output_act=\"relu\", edge_embedding_method='ip')(x_out)\n",
    "model = keras.Model(inputs=x_inp, outputs=prediction)\n",
    "\n",
    "model.compile(\n",
    "        optimizer=keras.optimizers.Adam(lr=1e-3),\n",
    "        loss=keras.losses.binary_crossentropy,\n",
    "        metrics=[\"acc\"],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:From /jet/var/python/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate array with shape (120000, 500) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-50d7023d9c71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m )\n",
      "\u001b[0;32m/jet/var/python/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m/jet/var/python/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m           \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m           steps_name='validation_steps')\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/jet/var/python/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtarget_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m       \u001b[0mbatch_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/jet/var/python/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36m_get_next_batch\u001b[0;34m(generator, mode)\u001b[0m\n\u001b[1;32m    360\u001b[0m   \u001b[0;34m\"\"\"Retrieves the next batch of input data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m     \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/jet/var/python/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 785\u001b[0;31m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/jet/var/python/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/jet/var/python/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    777\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/jet/var/python/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/jet/var/python/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwrap_exception\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_helper_reraises_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/jet/var/python/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget_index\u001b[0;34m(uid, i)\u001b[0m\n\u001b[1;32m    569\u001b[0m       \u001b[0mThe\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mat\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mi\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m   \"\"\"\n\u001b[0;32m--> 571\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_SHARED_SEQUENCES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/jet/var/python/lib/python3.6/site-packages/stellargraph/mapper/link_mappers.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, batch_num)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;31m# Get sampled nodes for GraphSAGELinkGenerator and HinSAGELinkGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             batch_feats = self.generator.sample_features(\n\u001b[0;32m--> 180\u001b[0;31m                 \u001b[0mhead_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampling_schema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             )\n\u001b[1;32m    182\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/jet/var/python/lib/python3.6/site-packages/stellargraph/mapper/link_mappers.py\u001b[0m in \u001b[0;36msample_features\u001b[0;34m(self, head_links, sampling_schema)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 [\n\u001b[1;32m    429\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_for_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mlayer_nodes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnodes_per_hop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m                 ]\n\u001b[1;32m    432\u001b[0m             )\n",
      "\u001b[0;32m/jet/var/python/lib/python3.6/site-packages/stellargraph/mapper/link_mappers.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 [\n\u001b[1;32m    429\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_for_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mlayer_nodes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnodes_per_hop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m                 ]\n\u001b[1;32m    432\u001b[0m             )\n",
      "\u001b[0;32m/jet/var/python/lib/python3.6/site-packages/stellargraph/core/graph.py\u001b[0m in \u001b[0;36mget_feature_for_nodes\u001b[0;34m(self, nodes, node_type)\u001b[0m\n\u001b[1;32m    530\u001b[0m             )\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_node_attribute_arrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate array with shape (120000, 500) and data type float64"
     ]
    }
   ],
   "source": [
    "#init_train_metrics = model.evaluate_generator(train_gen)\n",
    "#init_test_metrics = model.evaluate_generator(test_gen)\n",
    "#\n",
    "#print(\"\\nTrain Set Metrics of the initial (untrained) model:\")\n",
    "#for name, val in zip(model.metrics_names, init_train_metrics):\n",
    "#    print(\"\\t{}: {:0.4f}\".format(name, val))\n",
    "#\n",
    "#print(\"\\nTest Set Metrics of the initial (untrained) model:\")\n",
    "#for name, val in zip(model.metrics_names, init_test_metrics):\n",
    "#    print(\"\\t{}: {:0.4f}\".format(name, val))\n",
    "    \n",
    "history = model.fit_generator(\n",
    "    train_gen,\n",
    "    epochs=epochs,\n",
    "    validation_data=test_gen,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
