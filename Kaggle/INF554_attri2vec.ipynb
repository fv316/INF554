{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inductive Node Representation Learning through attri2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the python implementation of the attri2vec algorithm outlined in paper ***[Attributed Network Embedding Via Subspace Discovery](https://arxiv.org/abs/1901.04095)*** D. Zhang, Y. Jie, X. Zhu and C. Zhang, arXiv:1901.04095, [cs.SI], 2019. The implementation uses the stellargraph libraries.\n",
    "\n",
    "## attri2vec\n",
    "\n",
    "attri2vec learns node representations by performing a linear/non-linear mapping on node content attributes. To make the learned node representations respect structural similarity, [`DeepWalk`](https://dl.acm.org/citation.cfm?id=2623732)/[`node2vec`](https://snap.stanford.edu/node2vec) learning mechanism is used to make nodes sharing similar random walk context nodes represented closely in the subspace, which is achieved by maximizing the occurrence probability of context nodes conditioned on the representation of the target nodes. The probability is modelled by Softmax and negative sampling is used to speed up its calculation. This makes attri2vec equivalent to predict whether a node occurs in the given target node's context in random walks with the representation of the target node, by minimizing the cross-entropy loss. \n",
    "\n",
    "In implementation, node embeddings are learnt by solving a simple classification task: given a large set of \"positive\" `(target, context)` node pairs generated from random walks performed on the graph (i.e., node pairs that co-occur within a certain context window in random walks), and an equally large set of \"negative\" node pairs that are randomly selected from the graph according to a certain distribution, learn a binary classifier that predicts whether arbitrary node pairs are likely to co-occur in a random walk performed on the graph. Through learning this simple binary node-pair-classification task, the model automatically learns an inductive mapping from attributes of nodes to node embeddings in a low-dimensional vector space, which preserves structural and feature similarities of the nodes.\n",
    "\n",
    "To train the attri2vec model, we first construct a training set of nodes, which is composed of an equal number of positive and negative `(target, context)` pairs from the graph. The positive `(target, context)` pairs are the node pairs co-occurring on random walks over the graph whereas the negative node pairs are the sampled randomly from the global node degree distribution of the graph. In attri2vec, each node is attached with two kinds of embeddings: 1) the inductive 'input embedding', i.e, the objective embedding, obtained by perform a non-linear transformation on node content features, and 2) 'output embedding', i.e., the parameter vector used to predict its occurrence as a context node, obtained by looking up a parameter table. Given a `(target, context)` pair, attri2vec outputs a predictive value to indicate whether it is positive or negative, which is obtained by performing the dot product of the 'input embedding' of the target node and the 'output embedding' of the context node, followed by a sigmoid activation. \n",
    "\n",
    "The entire model is trained end-to-end by minimizing the binary cross-entropy loss function with regards to predicted node pair labels and true node pair labels, using stochastic gradient descent (SGD) updates of the model parameters, with minibatches of 'training' node pairs generated on demand and fed into the model.\n",
    "\n",
    "In this demo, we first train the attri2vec model on the in-sample subgraph and obtain a mapping function from node attributes to node representations, then apply the mapping function to the content attributes of out-of-sample nodes and obtain the representations of out-of-sample nodes. We evaluate the quality of inferred out-of-sample node representations by using it to predict the links of out-of-sample nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "import stellargraph as sg\n",
    "from stellargraph.data import UnsupervisedSampler\n",
    "from stellargraph.mapper import Attri2VecLinkGenerator, Attri2VecNodeGenerator\n",
    "from stellargraph.layer import Attri2Vec, link_classification\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import os\n",
    "import networkx as nx\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading DBLP network data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demo uses a DBLP citation network, a subgraph extracted from [DBLP-Citation-network V3](https://aminer.org/citation). To form this subgraph, papers from four subjects are extracted according to their venue information: *Database, Data Mining, Artificial Intelligence and Computer Vision*, and papers with no citations are removed. The DBLP network contains 18,448 papers and 45,661 citation relations. From paper titles, we construct 2,476-dimensional binary node feature vectors, with each element indicating the presence/absence of the corresponding word. By ignoring the citation direction, we take the DBLP subgraph as an undirected network.\n",
    "\n",
    "As papers in DBLP are attached with publication year, the DBLP network with the dynamic property can be used to study the problem of out-of-sample node representation learning. From the DBLP network, we construct four in-sample subgraphs using papers published before 2006, 2007, 2008 and 2009, and denote the four subgraphs as DBLP2006, DBLP2007, DBLP2008, and DBLP2009. For each subgraph, the remaining papers are taken as out-of-sample nodes. We consider the case where new coming nodes have no links. We predict the links of out-of-sample nodes using the learned out-of-sample node representations and compare its performance with the node content feature baseline.\n",
    "\n",
    "The dataset used in this demo can be downloaded from https://www.kaggle.com/daozhang/dblp-subgraph.\n",
    "The following is the description of the dataset:\n",
    "\n",
    "> The content.txt file contains descriptions of the papers in the following format:\n",
    "\n",
    " \t\t<paper_id> <word_attributes> <class_label> <publication_year>\n",
    "        \n",
    "> The first entry in each line contains the unique integer ID (ranging from 0 to 18,447) of the paper followed by binary values indicating whether each word in the vocabulary is present (indicated by 1) or absent (indicated by 0) in the paper. Finally, the last two entries in the line are the class label and the publication year of the paper.\n",
    "> The edgeList.txt file contains the citation relations. Each line describes a link in the following format:\n",
    "\t\t\n",
    "        <ID of paper1> <ID of paper2>\n",
    "        \n",
    "> Each line contains two paper IDs, with paper2 citing paper1 or paper1 citing paper2.\n",
    "\n",
    "\n",
    "Download and unzip the dblp-subgraph.zip file to a location on your computer and set the `data_dir` variable to\n",
    "point to the location of the dataset (the \"DBLP\" directory containing \"content.txt\" and \"edgeList.txt\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples shape: (453797, 3)\n",
      "Testing examples shape: (113450, 2)\n"
     ]
    }
   ],
   "source": [
    "with open(r\"training.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    training  = list(reader)\n",
    "# in order of training examples\n",
    "training = [element[0].split(\" \") for element in training]\n",
    "training = pd.DataFrame(training, columns=['Node1', 'Node2', 'Link'])\n",
    "print(\"Training examples shape: {}\".format(training.shape))\n",
    "\n",
    "with open(r\"testing.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    testing  = list(reader)\n",
    "# in order of testing examples\n",
    "testing = [element[0].split(\" \") for element in testing]\n",
    "testing = pd.DataFrame(testing, columns=['Node1', 'Node2'])\n",
    "print(\"Testing examples shape: {}\".format(testing.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "uncomment lines for reduced corpus with stopword removal. In future integrate stemmer here, multi-language\n",
    "'''\n",
    "NODE_INFO_DIRECTORY = r\"node_information/text/\"\n",
    "\n",
    "corpus_path = r\"pickles/simple_corpus.PICKLE\" \n",
    "ids_path = r\"pickles/ids.PICKLE\"\n",
    "if os.path.exists(corpus_path):\n",
    "    with open(corpus_path, 'rb') as f:\n",
    "        corpus = pickle.load(f)\n",
    "    f.close()\n",
    "    with open(ids_path, 'rb') as f:\n",
    "        ids = pickle.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    corpus = []\n",
    "    ids = []\n",
    "    for filename in tqdm(os.listdir(NODE_INFO_DIRECTORY), position=0, leave=True):\n",
    "        with open(NODE_INFO_DIRECTORY + filename, 'r', encoding='UTF-8', errors='ignore') as f:\n",
    "            doc_string = []\n",
    "            for line in f:\n",
    "                [doc_string.append(token.strip()) for token in line.lower().strip().split(\" \") if token != \"\"]\n",
    "            corpus.append(' '.join(doc_string))\n",
    "            ids.append(filename[:-4])\n",
    "    with open(corpus_path, '+wb') as f:\n",
    "        pickle.dump(corpus, f)\n",
    "    f.close()\n",
    "    with open(ids_path, '+wb') as f:\n",
    "        pickle.dump(ids, f)\n",
    "    f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training node info shape: (33226, 3)\n"
     ]
    }
   ],
   "source": [
    "stemmed_corpus_path = r\"pickles/stemmed_corpus.PICKLE\" \n",
    "if os.path.exists(stemmed_corpus_path):\n",
    "    with open(stemmed_corpus_path, 'rb') as f:\n",
    "        stemmed_corpus = pickle.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    print('Stemmed corpus unavailable')\n",
    "\n",
    "# in order of alphabetical text information i.e. 0, 1, 10, 100\n",
    "node_info = pd.DataFrame({'id': ids, 'corpus': corpus, 'stemmed': stemmed_corpus})\n",
    "node_info_id = node_info.set_index(['id'])\n",
    "print(\"Training node info shape: {}\".format(node_info.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph_split_path = 'pickles/train_graph_split.PICKLE'\n",
    "\n",
    "if os.path.exists(train_graph_split_path):\n",
    "    with open(train_graph_split_path, 'rb') as f:\n",
    "        keep_indices = pickle.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    keep_indices = random.sample(range(len(training)), k=int(len(training) * 0.05))\n",
    "    with open(train_graph_split_path, '+wb') as f:\n",
    "        pickle.dump(keep_indices, f)\n",
    "    f.close()\n",
    "\n",
    "data_train_val = training.iloc[keep_indices]\n",
    "data_train = training.loc[~training.index.isin(keep_indices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_nodes = training.loc[training['Link']=='1']\n",
    "linked_nodes = linked_nodes[['Node1', 'Node2']]\n",
    "edgelist = linked_nodes.rename(columns={\"Node1\": \"source\", \"Node2\": \"target\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the graph from the edgelist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_path = r\"pickles/stemmed_lda_matrix.PICKLE\"\n",
    "if os.path.exists(lda_path):\n",
    "    with open(lda_path, 'rb') as f:\n",
    "        lda = pickle.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    print(\"Fitting LDA models with tf features, \"\n",
    "          \"n_samples=%d and n_features=%d...\"\n",
    "          % (n_samples, n_features))\n",
    "    lda = LatentDirichletAllocation(n_components=n_features, max_iter=5,\n",
    "                                    learning_method='online',\n",
    "                                    learning_offset=50.,\n",
    "                                    random_state=0).fit_transform(corpus_word_matrix)\n",
    "    \n",
    "    with open(lda_path, '+wb') as f:\n",
    "        pickle.dump(lda, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33226, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load paper content features, subjects and publishing years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = node_column_names = [\"w_{}\".format(ii) for ii in range(10)]\n",
    "node_data = pd.DataFrame(lda, columns=node_column_names)\n",
    "node_data.index = [str(i) for i in node_data.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the whole graph from edge list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_all_nx = nx.from_pandas_edgelist(edgelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify node types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get node features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_node_features = node_data[feature_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Stellargraph with node features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_all = sg.StellarGraph(G_all_nx, node_features=all_node_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StellarGraph: Undirected multigraph\n",
      " Nodes: 33162, Edges: 283623\n",
      "\n",
      " Node types:\n",
      "  default: [33162]\n",
      "    Edge types: default-default->default\n",
      "\n",
      " Edge types:\n",
      "    default-default->default: [283623]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(G_all.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.23658547e-04, 3.23632150e-04, 3.23627755e-04, 3.23628454e-04,\n",
       "        3.23631539e-04, 3.23654211e-04, 3.23644432e-04, 3.23627872e-04,\n",
       "        9.97087240e-01, 3.23626678e-04]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_all.get_feature_for_nodes(['0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get DBLP Subgraph \n",
    "### with papers published before a threshold year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the edge list connecting in-sample nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_linked_nodes = data_train_val.loc[data_train_val['Link']=='1']\n",
    "sub_linked_nodes = sub_linked_nodes[['Node1', 'Node2']]\n",
    "subgraph_edgelist = sub_linked_nodes.rename(columns={\"Node1\": \"source\", \"Node2\": \"target\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the network from the selected edge list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_sub_nx = nx.from_pandas_edgelist(subgraph_edgelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify node types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the ids of the nodes in the selected subgraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph_node_ids = sorted(list(G_sub_nx.nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the node features of the selected subgraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph_node_features = node_data[feature_names].reindex(subgraph_node_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Stellargraph with node features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_sub = sg.StellarGraph(G_sub_nx, node_features=subgraph_node_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StellarGraph: Undirected multigraph\n",
      " Nodes: 12372, Edges: 14188\n",
      "\n",
      " Node types:\n",
      "  default: [12372]\n",
      "    Edge types: default-default->default\n",
      "\n",
      " Edge types:\n",
      "    default-default->default: [14188]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(G_sub.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train attri2vec on the DBLP Subgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the other optional parameter values: root nodes, the number of walks to take per node, the length of each walk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = list(G_sub.nodes())\n",
    "number_of_walks = 20\n",
    "length = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the UnsupervisedSampler instance with the relevant parameters passed to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsupervised_samples = UnsupervisedSampler(G_sub, nodes=nodes, length=length, number_of_walks=number_of_walks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the batch size and the number of epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an attri2vec training generator, which generates a batch of (feature of target node, index of context node, label of node pair) pairs per iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Attri2VecLinkGenerator(G_sub, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the model: a 1-hidden-layer node representation ('input embedding') of the `target` node and the parameter vector ('output embedding') for predicting the existence of `context node` for each `(target context)` pair, with a link classification layer performed on the dot product of the 'input embedding' of the `target` node and the 'output embedding' of the `context` node.\n",
    "\n",
    "Attri2Vec part of the model, with a 128-dimenssion hidden layer, no bias term, no dropout and no normalization. (Dropout can be switched on by specifying a positive dropout rate, 0 < dropout < 1 and normalization can be set to 'l2'). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Attri2VecLinkGenerator with an estimated 98976 batches generated on the fly per epoch.\n"
     ]
    }
   ],
   "source": [
    "layer_sizes = [128]\n",
    "attri2vec = Attri2Vec(layer_sizes=layer_sizes, generator=generator.flow(unsupervised_samples), bias=False, normalize=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\fvice\\Miniconda3\\envs\\map572\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\fvice\\Miniconda3\\envs\\map572\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# Build the model and expose input and output sockets of attri2vec, for node pair inputs:\n",
    "x_inp, x_out = attri2vec.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the link_classification function to generate the prediction, with the 'ip' edge embedding generation method and the 'sigmoid' activation, which actually performs the dot product of the 'input embedding' of the target node and the 'output embedding' of the context node followed by a sigmoid activation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "link_classification: using 'ip' method to combine node embeddings into edge embeddings\n"
     ]
    }
   ],
   "source": [
    "prediction = link_classification(\n",
    "    output_dim=1, output_act=\"sigmoid\", edge_embedding_method='ip'\n",
    ")(x_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stack the Attri2Vec encoder and prediction layer into a Keras model, and specify the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model(inputs=x_inp, outputs=prediction)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(lr=1e-2),\n",
    "    loss=keras.losses.binary_crossentropy,\n",
    "    metrics=[keras.metrics.binary_accuracy],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Attri2VecLinkGenerator with an estimated 98976 batches generated on the fly per epoch.\n",
      "WARNING:tensorflow:From C:\\Users\\fvice\\Miniconda3\\envs\\map572\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "98976/98976 - 2917s - loss: 0.6623 - binary_accuracy: 0.5960\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    generator.flow(unsupervised_samples),\n",
    "    epochs=epochs,\n",
    "    verbose=2,\n",
    "    use_multiprocessing=False,\n",
    "    workers=1,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_walks{}len{}e{}.h5'.format(number_of_walks, length, epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting links of out-of-sample nodes with the learned attri2vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the node based model for predicting node representations from node content attributes with the learned parameters. Below a Keras model is constructed, with x_inp[0] as input and x_out[0] as output. Note that this model's weights are the same as those of the corresponding node encoder in the previously trained node pair classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_inp_src = x_inp[0]\n",
    "x_out_src = x_out[0]\n",
    "embedding_model = keras.Model(inputs=x_inp_src, outputs=x_out_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_ids = list(node_data.index)\n",
    "listnodescustom = list(G_all.node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the node embeddings, for both in-sample and out-of-sample nodes, by applying the learned mapping function to node content features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28948\n"
     ]
    }
   ],
   "source": [
    "for i in node_ids:\n",
    "    if i not in listnodescustom:\n",
    "        print(i)\n",
    "        node_ids.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "664/664 [==============================] - ETA: 2: - ETA: 4s - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 1s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "node_gen = Attri2VecNodeGenerator(G_all, batch_size).flow(node_ids)\n",
    "node_embeddings = embedding_model.predict_generator(node_gen, workers=1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the positive and negative edges for in-sample nodes and out-of-sample nodes. The edges of the in-sample nodes only include the edges between in-sample nodes, and the edges of out-of-sample nodes are referred to all the edges linked to out-of-sample nodes, including the edges connecting in-sample and out-of-sample edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_thresh = 2006\n",
    "in_sample_edges = []\n",
    "out_of_sample_edges = []\n",
    "for ii in range(len(edgelist)):\n",
    "    source_index = edgelist[\"source\"][ii]\n",
    "    target_index = edgelist[\"target\"][ii]\n",
    "    if source_index > target_index: # neglect edge direction for the undirected graph\n",
    "        continue\n",
    "    source_year = int(node_data[\"year\"][source_index])\n",
    "    target_year = int(node_data[\"year\"][target_index])\n",
    "    if source_year < year_thresh and target_year < year_thresh:\n",
    "        in_sample_edges.append([source_index, target_index, 1]) # get the positive edge\n",
    "        negative_target_index = unsupervised_samples.random.choices(node_data.index.tolist(), k=1) # generate negative node\n",
    "        in_sample_edges.append([source_index, negative_target_index[0], 0]) # get the negative edge\n",
    "    else:\n",
    "        out_of_sample_edges.append([source_index, target_index, 1]) # get the positive edge\n",
    "        negative_target_index = unsupervised_samples.random.choices(node_data.index.tolist(), k=1) # generate negative node\n",
    "        out_of_sample_edges.append([source_index, negative_target_index[0], 0]) # get the negative edge\n",
    "in_sample_edges = np.array(in_sample_edges)\n",
    "out_of_sample_edges = np.array(out_of_sample_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the edge features from the learned node representations with l2 normed difference, where edge features are the element-wise square of the difference between the embeddings of two head nodes. Other strategy like element-wise product can also be used to construct edge features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_sample_edge_feat_from_emb = (node_embeddings[in_sample_edges[:,0]]-node_embeddings[in_sample_edges[:,1]])**2\n",
    "out_of_sample_edge_feat_from_emb = (node_embeddings[out_of_sample_edges[:,0]]-node_embeddings[out_of_sample_edges[:,1]])**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the Logistic Regression classifier from in-sample edges with the edge features constructed from attri2vec embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_edge_pred_from_emb = LogisticRegression(verbose=0, solver='lbfgs', multi_class=\"auto\", max_iter=500)\n",
    "clf_edge_pred_from_emb.fit(in_sample_edge_feat_from_emb, in_sample_edges[:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the edge existence probability with the trained Logistic Regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_pred_from_emb = clf_edge_pred_from_emb.predict_proba(out_of_sample_edge_feat_from_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the positive class index of `edge_pred_from_emb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if clf_edge_pred_from_emb.classes_[0] == 1:\n",
    "    positive_class_index = 0\n",
    "else:\n",
    "    positive_class_index = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the AUC score for the prediction with attri2vec embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6837179352491884"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(out_of_sample_edges[:,2], edge_pred_from_emb[:,positive_class_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the baseline, we also investigate the performance of node content features in predicting the edges of out-of-sample nodes. Firstly, we construct edge features from node content features with the same strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_sample_edge_rep_from_feat = (node_data[feature_names].values[in_sample_edges[:,0]]-node_data[feature_names].values[in_sample_edges[:,1]])**2\n",
    "out_of_sample_edge_rep_from_feat = (node_data[feature_names].values[out_of_sample_edges[:,0]]-node_data[feature_names].values[out_of_sample_edges[:,1]])**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we train the Logistic Regression classifier from in-sample edges with the edge features constructed from node content features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_edge_pred_from_feat = LogisticRegression(verbose=0, solver='lbfgs', multi_class=\"auto\", max_iter=500)\n",
    "clf_edge_pred_from_feat.fit(in_sample_edge_rep_from_feat, in_sample_edges[:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the edge existence probability with the trained Logistic Regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_pred_from_feat = clf_edge_pred_from_feat.predict_proba(out_of_sample_edge_rep_from_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get positive class index of `clf_edge_pred_from_feat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if clf_edge_pred_from_feat.classes_[0] == 1:\n",
    "    positive_class_index = 0\n",
    "else:\n",
    "    positive_class_index = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the AUC score for the prediction with node content features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.664362119314497"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(out_of_sample_edges[:,2], edge_pred_from_feat[:,positive_class_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attri2vec can inductively infer the representations of out-of-sample nodes from their content attributes. As the inferred node representations well capture both structure and node content information, they perform much better than node content features in predicting the links of out-of-sample nodes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
