{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INF554 Team ==Baseline== Language Detection and Text Stemming\n",
    "### Francisco, Alex and Aksel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import os\n",
    "import networkx as nx\n",
    "import pdb\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.feature_extraction import text as fe\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import time\n",
    "import keras\n",
    "import lightgbm\n",
    "import spacy\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from hyperopt import STATUS_OK, Trials, hp, space_eval, tpe, fmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training node info shape: (33226, 2)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "load the corpus one file at a time, remove spaces and convert to lower case\n",
    "'''\n",
    "NODE_INFO_DIRECTORY = r\"node_information/text/\"\n",
    "\n",
    "corpus_path = r\"pickles/simple_corpus.PICKLE\" \n",
    "ids_path = r\"pickles/ids.PICKLE\"\n",
    "if os.path.exists(corpus_path):\n",
    "    with open(corpus_path, 'rb') as f:\n",
    "        corpus = pickle.load(f)\n",
    "    f.close()\n",
    "    with open(ids_path, 'rb') as f:\n",
    "        ids = pickle.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    corpus = []\n",
    "    ids = []\n",
    "    for filename in tqdm(os.listdir(NODE_INFO_DIRECTORY), position=0, leave=True):\n",
    "        with open(NODE_INFO_DIRECTORY + filename, 'r', encoding='UTF-8', errors='ignore') as f:\n",
    "            doc_string = []\n",
    "            for line in f:\n",
    "                [doc_string.append(token.strip()) for token in line.lower().strip().split(\" \") if token != \"\"]\n",
    "            corpus.append(' '.join(doc_string))\n",
    "            ids.append(filename[:-4])\n",
    "    with open(corpus_path, '+wb') as f:\n",
    "        pickle.dump(corpus, f)\n",
    "    f.close()\n",
    "    with open(ids_path, '+wb') as f:\n",
    "        pickle.dump(ids, f)\n",
    "    f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "'''\n",
    "find the most likely languages in a document by counting the number of stop words for each language.\n",
    "Stopwords are those that are supported by the tokenizer\n",
    "'''\n",
    "def calculate_languages_ratios(text):\n",
    "    languages_ratios = []\n",
    "    tokens = wordpunct_tokenize(text)\n",
    "    words = [word.lower() for word in tokens]\n",
    "    for language in stopwords.fileids():\n",
    "        stopwords_set = set(stopwords.words(language))\n",
    "        words_set = set(words)\n",
    "        common_elements = words_set.intersection(stopwords_set)\n",
    "        languages_ratios.append(len(common_elements))\n",
    "    if sum(languages_ratios) == 0:\n",
    "        return np.zeros(len(languages_ratios))\n",
    "    return np.array(languages_ratios)/sum(languages_ratios)\n",
    "\n",
    "'''\n",
    "find the most likely languages in a tokenized document by counting the number of stop words for each language.\n",
    "Stopwords are those that are supported by the tokenizer and supported by the Snowball Stemmer. \n",
    "This function is used to stem the document and so will differ from the above (less supported languages)\n",
    "'''\n",
    "def calculate_languages_ratios_from_tokens(tokens):\n",
    "    languages_ratios = []\n",
    "    words = [word.lower() for word in tokens]\n",
    "    supported_languages = set(stopwords.fileids()) & set(SnowballStemmer.languages)\n",
    "    for language in supported_languages:\n",
    "        stopwords_set = set(stopwords.words(language))\n",
    "        words_set = set(words)\n",
    "        common_elements = words_set.intersection(stopwords_set)\n",
    "        languages_ratios.append(len(common_elements))\n",
    "    if sum(languages_ratios) == 0:\n",
    "        return np.zeros(len(languages_ratios))\n",
    "    return np.array(languages_ratios)/sum(languages_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 33226/33226 [17:04<00:00, 32.44it/s]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "for each document compute the language ratio\n",
    "Counting the stopwords is deterministic. Saved language ratios will be the same.\n",
    "'''\n",
    "languages_nltk_path = r\"pickles/languages_nltk.PICKLE\"\n",
    "if os.path.exists(languages_nltk_path):\n",
    "    with open(languages_nltk_path, 'rb') as f:\n",
    "        languages_nltk = pickle.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    languages_nltk = {}\n",
    "    loop_size = len(node_info['Corpus'])\n",
    "    # some documents have no text and therefore will raise an error\n",
    "    for i in tqdm(range(loop_size), total=loop_size, leave=True, position=0):\n",
    "        try:\n",
    "            languages_nltk[node_info['ID'][i]] =  calculate_languages_ratios(node_info['Corpus'][i])\n",
    "        except:\n",
    "            print('Error: {}'.format(i))\n",
    "            languages_nltk.append(None)\n",
    "    \n",
    "    with open(languages_nltk_path, '+wb') as f:\n",
    "        pickle.dump(languages_nltk, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 33226/33226 [1:40:17<00:00,  5.52it/s]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "this cell stems the document according to the most likely languages. \n",
    "If there is/are predominant language (25% of all stopwords detected belong to one language) then add these to the language.\n",
    "Else take all languages with >10% stop word representation (good for multi-lingual texts)\n",
    "FInally if none of the above hold, take the language which has the most ammount of stopwords in the document.\n",
    "'''\n",
    "supported_languages = list(set(stopwords.fileids()) & set(SnowballStemmer.languages))\n",
    "stemmed_corpus = []\n",
    "for text in tqdm(node_info['Corpus'].values, position=0, leave=True):\n",
    "    tokens = word_tokenize(text)\n",
    "    ratio = calculate_languages_ratios_from_tokens(tokens)\n",
    "    if np.sum(ratio == 0):\n",
    "        pass\n",
    "    if np.any(ratio>=0.25):\n",
    "        indices = np.where(ratio >= 0.25)[0]\n",
    "        langs = [supported_languages[j] for j in indices]\n",
    "    elif np.all(ratio<0.25) and np.any(ratio>0.10):\n",
    "        indices = np.where(ratio > 0.10)[0]\n",
    "        langs = [supported_languages[j] for j in indices]\n",
    "    else:\n",
    "        langs = [supported_languages[np.argmax(ratio)]]\n",
    "\n",
    "    for lang in langs:\n",
    "        lang_stopwords = stopwords.words(lang)\n",
    "        stemmer = SnowballStemmer(lang)\n",
    "        tokens = [stemmer.stem(word) for word in tokens if (word not in lang_stopwords) and word.isalpha()]\n",
    "    stemmed_corpus.append(' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickles/stemmed_corpus.PICKLE', '+wb') as f:\n",
    "    pickle.dump(stemmed_corpus, f)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
