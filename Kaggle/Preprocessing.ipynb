{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing and Language Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import os\n",
    "import networkx as nx\n",
    "import pdb\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.feature_extraction import text as fe\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import time\n",
    "import keras\n",
    "import lightgbm\n",
    "import spacy\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from hyperopt import STATUS_OK, Trials, hp, space_eval, tpe, fmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training node info shape: (33226, 2)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "uncomment lines for reduced corpus with stopword removal. In future integrate stemmer here, multi-language\n",
    "'''\n",
    "NODE_INFO_DIRECTORY = r\"node_information/text/\"\n",
    "corpus = []\n",
    "ids = []\n",
    "\n",
    "corpus_path = r\"pickles/lang_stemmed_corpus.PICKLE\" \n",
    "ids_path = r\"pickles/IDs.PICKLE\"\n",
    "if os.path.exists(corpus_path):\n",
    "    with open(corpus_path, 'rb') as f:\n",
    "        corpus = pickle.load(f)\n",
    "    f.close()\n",
    "    with open(ids_path, 'rb') as f:\n",
    "        ids = pickle.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    for filename in tqdm(os.listdir(NODE_INFO_DIRECTORY)):\n",
    "        with open(NODE_INFO_DIRECTORY + filename, 'r', encoding='UTF-8', errors='ignore') as f:\n",
    "            doc_string = []\n",
    "            for line in f:\n",
    "                [doc_string.append(token) for token in line.lower().strip().split(\" \")]\n",
    "            corpus.append(' '.join(doc_string))\n",
    "            ids.append(filename[:-4])\n",
    "    with open(corpus_path, '+wb') as f:\n",
    "        pickle.dump(corpus, f)\n",
    "    f.close()\n",
    "    with open(ids_path, '+wb') as f:\n",
    "        pickle.dump(ids, f)\n",
    "    f.close() \n",
    "\n",
    "# in order of alphabetical text information i.e. 0, 1, 10, 100\n",
    "node_info = pd.DataFrame({'ID': ids, 'Corpus': corpus})\n",
    "node_info_ID = node_info.set_index(['ID'])\n",
    "print(\"Training node info shape: {}\".format(node_info.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def calculate_languages_ratios(text):\n",
    "    languages_ratios = []\n",
    "    tokens = wordpunct_tokenize(text)\n",
    "    words = [word.lower() for word in tokens]\n",
    "    for language in stopwords.fileids():\n",
    "        stopwords_set = set(stopwords.words(language))\n",
    "        words_set = set(words)\n",
    "        common_elements = words_set.intersection(stopwords_set)\n",
    "        languages_ratios.append(len(common_elements))\n",
    "    if sum(languages_ratios) == 0:\n",
    "        return np.zeros(len(languages_ratios))\n",
    "    return np.array(languages_ratios)/sum(languages_ratios)\n",
    "\n",
    "def calculate_languages_ratios_from_tokens(tokens):\n",
    "    languages_ratios = []\n",
    "    words = [word.lower() for word in tokens]\n",
    "    supported_languages = set(stopwords.fileids()) & set(SnowballStemmer.languages)\n",
    "    for language in supported_languages:\n",
    "        stopwords_set = set(stopwords.words(language))\n",
    "        words_set = set(words)\n",
    "        common_elements = words_set.intersection(stopwords_set)\n",
    "        languages_ratios.append(len(common_elements))\n",
    "    if sum(languages_ratios) == 0:\n",
    "        return np.zeros(len(languages_ratios))\n",
    "    return np.array(languages_ratios)/sum(languages_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 33226/33226 [17:04<00:00, 32.44it/s]\n"
     ]
    }
   ],
   "source": [
    "languages_nltk_path = r\"pickles/languages_nltk.PICKLE\"\n",
    "if os.path.exists(languages_nltk_path):\n",
    "    with open(languages_nltk_path, 'rb') as f:\n",
    "        languages_nltk = pickle.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    languages_nltk = {}\n",
    "    loop_size = len(node_info['Corpus'])\n",
    "    for i in tqdm(range(loop_size), total=loop_size, leave=True, position=0):\n",
    "        try:\n",
    "            languages_nltk[node_info['ID'][i]] =  calculate_languages_ratios(node_info['Corpus'][i])\n",
    "        except:\n",
    "            languages_nltk.append(None)\n",
    "    \n",
    "    with open(languages_nltk_path, '+wb') as f:\n",
    "        pickle.dump(languages_nltk, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 33226/33226 [1:40:17<00:00,  5.52it/s]\n"
     ]
    }
   ],
   "source": [
    "supported_languages = list(set(stopwords.fileids()) & set(SnowballStemmer.languages))\n",
    "stemmed_corpus = []\n",
    "for text in tqdm(node_info['Corpus'].values, position=0, leave=True):\n",
    "    tokens = word_tokenize(text)\n",
    "    ratio = calculate_languages_ratios_from_tokens(tokens)\n",
    "    if np.sum(ratio == 0):\n",
    "        pass\n",
    "    if np.any(ratio>=0.25):\n",
    "        indices = np.where(ratio >= 0.25)[0]\n",
    "        langs = [supported_languages[j] for j in indices]\n",
    "    elif np.all(ratio<0.25) and np.any(ratio>0.10):\n",
    "        indices = np.where(ratio > 0.10)[0]\n",
    "        langs = [supported_languages[j] for j in indices]\n",
    "    else:\n",
    "        langs = [supported_languages[np.argmax(ratio)]]\n",
    "\n",
    "    for lang in langs:\n",
    "        lang_stopwords = stopwords.words(lang)\n",
    "        stemmer = SnowballStemmer(lang)\n",
    "        tokens = [stemmer.stem(word) for word in tokens if (word not in lang_stopwords) and word.isalpha()]\n",
    "    stemmed_corpus.append(' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickles/stemmed_corpus.PICKLE', '+wb') as f:\n",
    "    pickle.dump(stemmed_corpus, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
