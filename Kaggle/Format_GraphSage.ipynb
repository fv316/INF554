{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting and Downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples shape: (453797, 3)\n",
      "Testing examples shape: (113450, 2)\n"
     ]
    }
   ],
   "source": [
    "with open(r\"training.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    training  = list(reader)\n",
    "# in order of training examples\n",
    "training = [element[0].split(\" \") for element in training]\n",
    "training = pd.DataFrame(training, columns=['Node1', 'Node2', 'Link'])\n",
    "print(\"Training examples shape: {}\".format(training.shape))\n",
    "\n",
    "with open(r\"testing.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    testing  = list(reader)\n",
    "# in order of testing examples\n",
    "testing = [element[0].split(\" \") for element in testing]\n",
    "testing = pd.DataFrame(testing, columns=['Node1', 'Node2'])\n",
    "print(\"Testing examples shape: {}\".format(testing.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "uncomment lines for reduced corpus with stopword removal. In future integrate stemmer here, multi-language\n",
    "'''\n",
    "NODE_INFO_DIRECTORY = r\"node_information/text/\"\n",
    "\n",
    "corpus_path = r\"pickles/simple_corpus.PICKLE\" \n",
    "ids_path = r\"pickles/ids.PICKLE\"\n",
    "if os.path.exists(corpus_path):\n",
    "    with open(corpus_path, 'rb') as f:\n",
    "        corpus = pickle.load(f)\n",
    "    f.close()\n",
    "    with open(ids_path, 'rb') as f:\n",
    "        ids = pickle.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    corpus = []\n",
    "    ids = []\n",
    "    for filename in tqdm(os.listdir(NODE_INFO_DIRECTORY), position=0, leave=True):\n",
    "        with open(NODE_INFO_DIRECTORY + filename, 'r', encoding='UTF-8', errors='ignore') as f:\n",
    "            doc_string = []\n",
    "            for line in f:\n",
    "                [doc_string.append(token.strip()) for token in line.lower().strip().split(\" \") if token != \"\"]\n",
    "            corpus.append(' '.join(doc_string))\n",
    "            ids.append(filename[:-4])\n",
    "    with open(corpus_path, '+wb') as f:\n",
    "        pickle.dump(corpus, f)\n",
    "    f.close()\n",
    "    with open(ids_path, '+wb') as f:\n",
    "        pickle.dump(ids, f)\n",
    "    f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training node info shape: (33226, 3)\n"
     ]
    }
   ],
   "source": [
    "stemmed_corpus_path = r\"pickles/stemmed_corpus.PICKLE\" \n",
    "if os.path.exists(stemmed_corpus_path):\n",
    "    with open(stemmed_corpus_path, 'rb') as f:\n",
    "        stemmed_corpus = pickle.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    print('Stemmed corpus unavailable')\n",
    "\n",
    "# in order of alphabetical text information i.e. 0, 1, 10, 100\n",
    "node_info = pd.DataFrame({'id': ids, 'corpus': corpus, 'stemmed': stemmed_corpus})\n",
    "node_info_id = node_info.set_index(['id'])\n",
    "print(\"Training node info shape: {}\".format(node_info.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph_split_path = 'pickles/train_graph_split.PICKLE'\n",
    "\n",
    "if os.path.exists(train_graph_split_path):\n",
    "    with open(train_graph_split_path, 'rb') as f:\n",
    "        keep_indices = pickle.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    keep_indices = random.sample(range(len(training)), k=int(len(training) * 0.05))\n",
    "    with open(train_graph_split_path, '+wb') as f:\n",
    "        pickle.dump(keep_indices, f)\n",
    "    f.close()\n",
    "\n",
    "data_train_val = training.iloc[keep_indices]\n",
    "data_train = training.loc[~training.index.isin(keep_indices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_path = r\"pickles/stemmed_lda_64_matrix.PICKLE\"\n",
    "if os.path.exists(lda_path):\n",
    "    with open(lda_path, 'rb') as f:\n",
    "        lda = pickle.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    print(\"Fitting LDA models with tf features, \"\n",
    "          \"n_samples=%d and n_features=%d...\"\n",
    "          % (n_samples, n_features))\n",
    "    lda = LatentDirichletAllocation(n_components=n_features, max_iter=5,\n",
    "                                    learning_method='online',\n",
    "                                    learning_offset=50.,\n",
    "                                    random_state=0).fit_transform(corpus_word_matrix)\n",
    "    \n",
    "    with open(lda_path, '+wb') as f:\n",
    "        pickle.dump(lda, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting for Graph Sage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_nodes = training.loc[training['Link']=='1']\n",
    "linked_nodes = linked_nodes[['Node1', 'Node2']]\n",
    "linked_nodes.to_csv('all_linked_nodes.txt', sep=' ', index=False, header=False)\n",
    "G=nx.read_edgelist('all_linked_nodes.txt', create_using=nx.Graph(), nodetype = int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding: 2015\n",
      "adding: 5703\n",
      "adding: 10049\n",
      "adding: 12990\n",
      "adding: 17120\n",
      "adding: 20596\n",
      "adding: 21465\n",
      "adding: 21986\n",
      "adding: 22507\n",
      "adding: 23438\n",
      "adding: 23525\n",
      "adding: 23709\n",
      "adding: 24663\n",
      "adding: 25156\n",
      "adding: 25283\n",
      "adding: 25499\n",
      "adding: 25516\n",
      "adding: 25601\n",
      "adding: 25769\n",
      "adding: 26210\n",
      "adding: 26897\n",
      "adding: 26905\n",
      "adding: 27435\n",
      "adding: 27520\n",
      "adding: 27534\n",
      "adding: 27553\n",
      "adding: 28061\n",
      "adding: 28360\n",
      "adding: 28372\n",
      "adding: 28823\n",
      "adding: 28860\n",
      "adding: 28936\n",
      "adding: 28944\n",
      "adding: 28947\n",
      "adding: 28948\n",
      "adding: 29048\n",
      "adding: 29280\n",
      "adding: 30474\n",
      "adding: 30504\n",
      "adding: 30556\n",
      "adding: 30660\n",
      "adding: 30697\n",
      "adding: 30766\n",
      "adding: 30829\n",
      "adding: 30874\n",
      "adding: 30959\n",
      "adding: 31007\n",
      "adding: 31081\n",
      "adding: 31140\n",
      "adding: 31157\n",
      "adding: 31377\n",
      "adding: 31785\n",
      "adding: 31995\n",
      "adding: 32447\n",
      "adding: 32605\n",
      "adding: 32806\n",
      "adding: 32864\n",
      "adding: 33100\n",
      "adding: 33113\n",
      "adding: 33165\n",
      "adding: 33175\n",
      "adding: 33199\n",
      "adding: 33213\n",
      "adding: 33221\n"
     ]
    }
   ],
   "source": [
    "for i in (list(node_info.index)):\n",
    "    if i in G:\n",
    "        G.node[i]['feature'] = list(lda[i])       \n",
    "        G.node[i]['test'] = False\n",
    "        G.node[i]['val'] = False\n",
    "\n",
    "    else:\n",
    "        print('adding: ' + str(i))\n",
    "        G.add_node((i))\n",
    "        G.node[(i)]['feature'] = list(lda[i])\n",
    "        G.node[(i)]['test'] = False\n",
    "        G.node[(i)]['val'] = False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = data_train_val.index[1]\n",
    "G.edge[int(data_train_val['Node1'].loc[i])][int(data_train_val['Node2'].loc[i])]['test_removed'] = False\n",
    "G.edge[int(data_train_val['Node1'].loc[i])][int(data_train_val['Node2'].loc[i])]['train_removed'] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding: 74300\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 74300 is out of bounds for axis 0 with size 33226",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-f98cf23e86e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'adding: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_node\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'feature'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 74300 is out of bounds for axis 0 with size 33226"
     ]
    }
   ],
   "source": [
    "for i in (list(data_train_val.index)):\n",
    "    if i in G:\n",
    "        G.node[i]['feature'] = list(lda[i])       \n",
    "        G.node[i]['test'] = False\n",
    "        G.node[i]['val'] = False\n",
    "\n",
    "    else:\n",
    "        print('adding: ' + str(i))\n",
    "        G.add_node((i))\n",
    "        G.node[(i)]['feature'] = list(lda[i])\n",
    "        G.node[(i)]['test'] = False\n",
    "        G.node[(i)]['val'] = False\n",
    "        \n",
    "for i in (list(data_train.index)):\n",
    "    if i in G:\n",
    "        G.node[i]['feature'] = list(lda[i])       \n",
    "        G.node[i]['test'] = False\n",
    "        G.node[i]['val'] = False\n",
    "\n",
    "    else:\n",
    "        print('adding: ' + str(i))\n",
    "        G.add_node((i))\n",
    "        G.node[(i)]['feature'] = list(lda[i])\n",
    "        G.node[(i)]['test'] = False\n",
    "        G.node[(i)]['val'] = False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_id_map = {}\n",
    "binaries = int(math.log(len(node_info_id.index), 2)) + 1\n",
    "for j in list(node_info_id.index):\n",
    "    bin_repr = bin(int(j))[2:]\n",
    "    dict_id_map[j] = [int(i) for i in ((16 - len(bin_repr))*['0'] + list(bin_repr))]\n",
    "with open('example-class_map.json', '+w') as f:\n",
    "    json.dump(dict_id_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_id_map = {}\n",
    "for i, j in enumerate(list(node_info_id.index)):\n",
    "    dict_id_map[j] = int(j)\n",
    "with open('example-id_map.json', '+w') as f:\n",
    "    json.dump(dict_id_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.readwrite import json_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json_graph.node_link_data(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('example-feats.npy', np.array(lda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('example-G.json', '+w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"example\"\n",
    "G_data = json.load(open(prefix + \"-G.json\"))\n",
    "G = json_graph.node_link_graph(G_data)\n",
    "if isinstance(G.nodes()[0], int):\n",
    "    conversion = lambda n : int(n)\n",
    "else:\n",
    "    conversion = lambda n : n\n",
    "\n",
    "if os.path.exists(prefix + \"-feats.npy\"):\n",
    "    feats = np.load(prefix + \"-feats.npy\")\n",
    "else:\n",
    "    print(\"No features present.. Only identity features will be used.\")\n",
    "    feats = None\n",
    "id_map = json.load(open(prefix + \"-id_map.json\"))\n",
    "id_map = {conversion(k):int(v) for k,v in id_map.items()}\n",
    "walks = []\n",
    "class_map = json.load(open(prefix + \"-class_map.json\"))\n",
    "if isinstance(list(class_map.values())[0], list):\n",
    "    lab_conversion = lambda n : n\n",
    "else:\n",
    "    lab_conversion = lambda n : int(n)\n",
    "\n",
    "class_map = {conversion(k):lab_conversion(v) for k,v in class_map.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove all nodes that do not have val/test annotations\n",
    "## (necessary because of networkx weirdness with the Reddit data)\n",
    "broken_count = 0\n",
    "for node in G.nodes():\n",
    "    if (not 'val' in G.node[node]) or (not 'test' in G.node[node]):\n",
    "        G.remove_node(node)\n",
    "        broken_count += 1\n",
    "print(\"Removed {:d} nodes that lacked proper annotations due to networkx versioning issues\".format(broken_count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
