{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Packages \n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm # progess bar\n",
    "import pickle\n",
    "\n",
    "# For graphs\n",
    "import networkx as nx \n",
    "import stellargraph as sg\n",
    "from stellargraph.data import EdgeSplitter\n",
    "from stellargraph.mapper import GraphSAGELinkGenerator\n",
    "from stellargraph.layer import GraphSAGE, HinSAGE, link_classification\n",
    "from stellargraph import globalvar\n",
    "\n",
    "# For DL\n",
    "from tensorflow import keras \n",
    "######\n",
    "import tensorflow as tf\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=4, inter_op_parallelism_threads=4, allow_soft_placement=True, device_count = {'CPU': 4})\n",
    "\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"4\"\n",
    "\n",
    "os.environ[\"KMP_BLOCKTIME\"] = \"30\"\n",
    "\n",
    "os.environ[\"KMP_SETTINGS\"] = \"1\"\n",
    "\n",
    "os.environ[\"KMP_AFFINITY\"]= \"granularity=fine,verbose,compact,1,0\"\n",
    "\n",
    "#####\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# For processing node texts\n",
    "#import spacy\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.feature_extraction import text as fe\n",
    "\n",
    "# Dimensionality reduction\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "# Word embeddings\n",
    "import gensim \n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# For stemming\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.gpu_device_name()\n",
    "#!cat /proc/meminfo\n",
    "!cat /proc/cpuinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load corpus and ids from pickles\n",
    "corpus_path = r\"pickles/corpus.PICKLE\" \n",
    "ids_path = r\"pickles/IDs.PICKLE\"\n",
    "with open(corpus_path, 'rb') as f:\n",
    "    corpus = pickle.load(f)\n",
    "f.close()\n",
    "with open(ids_path, 'rb') as f:\n",
    "    ids = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# Save in dataframe\n",
    "node_info = pd.DataFrame({'ID': ids, 'Corpus': corpus})\n",
    "node_info_ID = node_info.set_index(['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each set of tokens calculate ratio of each language present\n",
    "def calculate_languages_ratios_from_tokens(tokens):\n",
    "    languages_ratios = []\n",
    "    \n",
    "    # Lower words in set of tokens\n",
    "    words = [word.lower() for word in tokens]\n",
    "    \n",
    "    # Supported languages as intersection\n",
    "    supported_languages = set(stopwords.fileids()) & set(SnowballStemmer.languages)\n",
    "    \n",
    "    # For each language, identify ratio in set of tokens\n",
    "    for language in supported_languages:\n",
    "        stopwords_set = set(stopwords.words(language))\n",
    "        words_set = set(words)\n",
    "        common_elements = words_set.intersection(stopwords_set)\n",
    "        languages_ratios.append(len(common_elements))\n",
    "        \n",
    "    # Set to zero if ratio is zero\n",
    "    if sum(languages_ratios) == 0:\n",
    "        return np.zeros(len(languages_ratios))\n",
    "    \n",
    "    return np.array(languages_ratios)/sum(languages_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stem corpus based on frequent languages\n",
    "\n",
    "# Defining path\n",
    "stemmed_corpus_path = r\"pickles/stemmed_corpus.PICKLE\"\n",
    "\n",
    "\n",
    "if os.path.exists(stemmed_corpus_path):\n",
    "    pass\n",
    "else:\n",
    "    # Supported languages as intersection\n",
    "    supported_languages = list(set(stopwords.fileids()) & set(SnowballStemmer.languages))\n",
    "    stemmed_corpus = []\n",
    "\n",
    "    # For each text in corpus\n",
    "    for text in tqdm(node_info['Corpus'].values, position=0, leave=True):\n",
    "\n",
    "        # Identify tokens\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # Compute language ratios\n",
    "        ratio = calculate_languages_ratios_from_tokens(tokens)\n",
    "\n",
    "        # Note most frequent languages in langs\n",
    "        if np.sum(ratio == 0):\n",
    "            pass\n",
    "        if np.any(ratio>=0.25):\n",
    "            indices = np.where(ratio >= 0.25)[0]\n",
    "            langs = [supported_languages[j] for j in indices]\n",
    "        elif np.all(ratio<0.25) and np.any(ratio>0.10):\n",
    "            indices = np.where(ratio > 0.10)[0]\n",
    "            langs = [supported_languages[j] for j in indices]\n",
    "        else:\n",
    "            langs = [supported_languages[np.argmax(ratio)]]\n",
    "\n",
    "        # For each frequent language stem word if not a stopword \n",
    "        # and if it consists of alphabet letters\n",
    "        for lang in langs:\n",
    "            lang_stopwords = stopwords.words(lang)\n",
    "            stemmer = SnowballStemmer(lang)\n",
    "            tokens = [stemmer.stem(word) for word in tokens if (word not in lang_stopwords) and word.isalpha()]\n",
    "        stemmed_corpus.append(' '.join(tokens))\n",
    "\n",
    "    # Dump pickle\n",
    "    with open(stemmed_corpus_path, '+wb') as f:\n",
    "        pickle.dump(stemmed_corpus, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generating smaller dictionary with stemmed words, no stopwords and frequency > 20\n",
    "\n",
    "# Defining paths\n",
    "small_matrix_path = r\"pickles/small_word_matrix.PICKLE\"\n",
    "corpus_path = r\"pickles/stemmed_corpus.PICKLE\"\n",
    "\n",
    "if os.path.exists(small_matrix_path):\n",
    "    with open(small_matrix_path, 'rb') as f:\n",
    "        word_matrix = pickle.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    with open(corpus_path, 'rb') as f:\n",
    "        stemmed_corpus = pickle.load(f)\n",
    "        \n",
    "        # Get vectorizer from feature extraction package\n",
    "        vectorizer = fe.CountVectorizer(min_df = 20, max_df = 0.9, strip_accents = 'unicode')\n",
    "        \n",
    "        # Vectorize corpus\n",
    "        word_matrix = vectorizer.fit_transform(tqdm(stemmed_corpus))\n",
    "        \n",
    "        # Dump pickle\n",
    "        with open(small_matrix_path, '+wb') as g:\n",
    "            pickle.dump(word_matrix, g)\n",
    "        g.close()\n",
    "    f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_matrix_path = r\"pickles/corpus_tfidf_matrix.PICKLE\"\n",
    "if os.path.exists(corpus_matrix_path):\n",
    "    with open(corpus_matrix_path, 'rb') as f:\n",
    "        corpus_tfidf_matrix = pickle.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    with open(corpus_path, 'rb') as g:\n",
    "        stemmed_corpus = pickle.load(g)\n",
    "        vectorizer3 = fe.TfidfVectorizer(min_df = 20, max_df = 0.9, strip_accents = 'unicode')\n",
    "        corpus_tfidf_matrix = vectorizer3.fit_transform(tqdm(stemmed_corpus))\n",
    "        with open(corpus_matrix_path, '+wb') as f:\n",
    "            pickle.dump(corpus_tfidf_matrix, f)\n",
    "        f.close()\n",
    "    g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = corpus_tfidf_matrix.shape[1]\n",
    "n_components = 100\n",
    "\n",
    "nmf_frobenius_path = r\"pickles/nmf_frobenius_matrix.PICKLE\"\n",
    "if os.path.exists(nmf_frobenius_path):\n",
    "    with open(nmf_frobenius_path, 'rb') as f:\n",
    "        nmf_frobenius = pickle.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
    "          \"n_samples=%d and n_features=%d...\" % (corpus_tfidf_matrix.shape[0], n_features))\n",
    "\n",
    "    nmf_frobenius = NMF(n_components=n_components, random_state=1, alpha=.1, l1_ratio=.5).fit(corpus_tfidf_matrix)\n",
    "    with open(nmf_frobenius_path, '+wb') as f:\n",
    "        pickle.dump(nmf_frobenius, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_reduced_matrix_path = r\"pickles/nmf_frobenius_matrix_realized.PICKLE\"\n",
    "if os.path.exists(small_reduced_matrix_path):\n",
    "    with open(small_reduced_matrix_path, 'rb') as f:\n",
    "        small_reduced_matrix = pickle.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    with open(small_reduced_matrix_path, '+wb') as f:\n",
    "        small_reduced_matrix = nmf_frobenius.fit_transform(corpus_tfidf_matrix)\n",
    "        pickle.dump(small_reduced_matrix, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA models with tf features, n_samples=33226 and n_features=42691...\n"
     ]
    }
   ],
   "source": [
    "lda_path = r\"pickles/lda_matrix.PICKLE\"\n",
    "n_features = corpus_tfidf_matrix.shape[1]\n",
    "n_components = 100\n",
    "\n",
    "if os.path.exists(lda_path):\n",
    "    with open(lda_path, 'rb') as f:\n",
    "        lda_matrix = pickle.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    print(\"Fitting LDA models with tf features, \"\n",
    "          \"n_samples=%d and n_features=%d...\"\n",
    "          % (corpus_tfidf_matrix.shape[0], n_features))\n",
    "    lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                    learning_method='online',\n",
    "                                    learning_offset=50.,\n",
    "                                    random_state=0)\n",
    "    lda.fit(corpus_tfidf_matrix)\n",
    "    lda_matrix = lda.fit_transform(corpus_tfidf_matrix)\n",
    "    with open(lda_path, '+wb') as f:\n",
    "        pickle.dump(lda_matrix, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33226 100\n"
     ]
    }
   ],
   "source": [
    "# Storing in dataframe\n",
    "n1, n2 = lda_matrix.shape\n",
    "# Creating feature names\n",
    "feature_names = [\"w_{}\".format(ii) for ii in range(n2)]\n",
    "ids = sorted(range(n1), key=str)\n",
    "node_data = pd.DataFrame(data=lda_matrix, index=ids, columns=feature_names)\n",
    "print(n1,n2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make graphs great again\n",
    "\n",
    "Time to get training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples shape: (453797, 3)\n",
      "Testing examples shape: (113450, 2)\n"
     ]
    }
   ],
   "source": [
    "# Read training\n",
    "with open(r\"training.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    training  = list(reader)\n",
    "# in order of training examples\n",
    "training = [element[0].split(\" \") for element in training]\n",
    "training = pd.DataFrame(training, columns=['Node1', 'Node2', 'Link'])\n",
    "print(\"Training examples shape: {}\".format(training.shape))\n",
    "\n",
    "# Read testing\n",
    "with open(r\"testing.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    testing  = list(reader)\n",
    "# in order of testing examples\n",
    "testing = [element[0].split(\" \") for element in testing]\n",
    "testing = pd.DataFrame(testing, columns=['Node1', 'Node2'])\n",
    "print(\"Testing examples shape: {}\".format(testing.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_nodes = training.loc[training['Link']=='1']\n",
    "linked_nodes = linked_nodes[['Node1', 'Node2']]\n",
    "linked_nodes.to_csv('linked_nodes.txt', sep=' ', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 1000 edges\n",
      "Removed 2000 edges\n",
      "Removed 3000 edges\n",
      "Removed 4000 edges\n",
      "Removed 5000 edges\n",
      "Removed 6000 edges\n",
      "Removed 7000 edges\n",
      "Removed 8000 edges\n",
      "Removed 9000 edges\n",
      "Removed 10000 edges\n",
      "Removed 11000 edges\n",
      "Removed 12000 edges\n",
      "Removed 13000 edges\n",
      "Removed 14000 edges\n",
      "Removed 15000 edges\n",
      "Removed 16000 edges\n",
      "Removed 17000 edges\n",
      "Removed 18000 edges\n",
      "Removed 19000 edges\n",
      "Removed 20000 edges\n",
      "Removed 21000 edges\n",
      "Removed 22000 edges\n",
      "Removed 23000 edges\n",
      "Removed 24000 edges\n",
      "Removed 25000 edges\n",
      "Removed 26000 edges\n",
      "Removed 27000 edges\n",
      "Removed 28000 edges\n",
      "Sampled 1000 negative examples\n",
      "Sampled 2000 negative examples\n",
      "Sampled 3000 negative examples\n",
      "Sampled 4000 negative examples\n",
      "Sampled 5000 negative examples\n",
      "Sampled 6000 negative examples\n",
      "Sampled 7000 negative examples\n",
      "Sampled 8000 negative examples\n",
      "Sampled 9000 negative examples\n",
      "Sampled 10000 negative examples\n",
      "Sampled 11000 negative examples\n",
      "Sampled 12000 negative examples\n",
      "Sampled 13000 negative examples\n",
      "Sampled 14000 negative examples\n",
      "Sampled 15000 negative examples\n",
      "Sampled 16000 negative examples\n",
      "Sampled 17000 negative examples\n",
      "Sampled 18000 negative examples\n",
      "Sampled 19000 negative examples\n",
      "Sampled 20000 negative examples\n",
      "Sampled 21000 negative examples\n",
      "Sampled 22000 negative examples\n",
      "Sampled 23000 negative examples\n",
      "Sampled 24000 negative examples\n",
      "Sampled 25000 negative examples\n",
      "Sampled 26000 negative examples\n",
      "Sampled 27000 negative examples\n",
      "Sampled 28000 negative examples\n",
      "** Sampled 28362 positive and 28362 negative edges. **\n",
      "Removed 1000 edges\n",
      "Removed 2000 edges\n",
      "Removed 3000 edges\n",
      "Removed 4000 edges\n",
      "Removed 5000 edges\n",
      "Removed 6000 edges\n",
      "Removed 7000 edges\n",
      "Removed 8000 edges\n",
      "Removed 9000 edges\n",
      "Removed 10000 edges\n",
      "Removed 11000 edges\n",
      "Removed 12000 edges\n",
      "Removed 13000 edges\n",
      "Removed 14000 edges\n",
      "Removed 15000 edges\n",
      "Removed 16000 edges\n",
      "Removed 17000 edges\n",
      "Removed 18000 edges\n",
      "Removed 19000 edges\n",
      "Removed 20000 edges\n",
      "Removed 21000 edges\n",
      "Removed 22000 edges\n",
      "Removed 23000 edges\n",
      "Removed 24000 edges\n",
      "Removed 25000 edges\n",
      "Sampled 1000 negative examples\n",
      "Sampled 2000 negative examples\n",
      "Sampled 3000 negative examples\n",
      "Sampled 4000 negative examples\n",
      "Sampled 5000 negative examples\n",
      "Sampled 6000 negative examples\n",
      "Sampled 7000 negative examples\n",
      "Sampled 8000 negative examples\n",
      "Sampled 9000 negative examples\n",
      "Sampled 10000 negative examples\n",
      "Sampled 11000 negative examples\n",
      "Sampled 12000 negative examples\n",
      "Sampled 13000 negative examples\n",
      "Sampled 14000 negative examples\n",
      "Sampled 15000 negative examples\n",
      "Sampled 16000 negative examples\n",
      "Sampled 17000 negative examples\n",
      "Sampled 18000 negative examples\n",
      "Sampled 19000 negative examples\n",
      "Sampled 20000 negative examples\n",
      "Sampled 21000 negative examples\n",
      "Sampled 22000 negative examples\n",
      "Sampled 23000 negative examples\n",
      "Sampled 24000 negative examples\n",
      "Sampled 25000 negative examples\n",
      "** Sampled 25526 positive and 25526 negative edges. **\n"
     ]
    }
   ],
   "source": [
    "# Read edges and create NetworkX graph\n",
    "edgelist = pd.read_csv(\"linked_nodes.txt\", sep=' ', header=None, names=[\"source\", \"target\"])\n",
    "edgelist[\"label\"] = \"cites\"  # set the edge type\n",
    "G_all_nx = nx.from_pandas_edgelist(edgelist, edge_attr=\"label\")\n",
    "nx.set_node_attributes(G_all_nx, \"paper\", \"label\")\n",
    "\n",
    "# Initialize Stellargraph with node features of text\n",
    "#G_all = sg.StellarGraph(G_all_nx, node_features=node_data[feature_names])\n",
    "\n",
    "# Define an edge splitter on the original graph G:\n",
    "edge_splitter_test = EdgeSplitter(G_all_nx)\n",
    "\n",
    "# Randomly sample a fraction p=0.1 of all positive links, \n",
    "# and same number of negative links, from G, and obtain the\n",
    "# reduced graph G_test with the sampled links removed:\n",
    "G_test, edge_ids_test, edge_labels_test = edge_splitter_test.train_test_split(\n",
    "    p=0.1, method=\"global\", keep_connected=True)\n",
    "\n",
    "# Define an edge splitter on the reduced graph G_test:\n",
    "edge_splitter_train = EdgeSplitter(G_test)\n",
    "\n",
    "# Randomly sample a fraction p=0.1 of all positive links, and same number of negative links, from G_test, and obtain the\n",
    "# reduced graph G_train with the sampled links removed:\n",
    "G_train, edge_ids_train, edge_labels_train = edge_splitter_train.train_test_split(\n",
    "    p=0.1, method=\"global\", keep_connected=True)\n",
    "\n",
    "G_test = sg.StellarGraph(G_test, node_features=node_data[feature_names])\n",
    "G_train = sg.StellarGraph(G_train, node_features=node_data[feature_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StellarGraph: Undirected multigraph\n",
      " Nodes: 33162, Edges: 255261\n",
      "\n",
      " Node types:\n",
      "  paper: [33162]\n",
      "    Edge types: paper-cites->paper\n",
      "\n",
      " Edge types:\n",
      "    paper-cites->paper: [255261]\n",
      "\n",
      "StellarGraph: Undirected multigraph\n",
      " Nodes: 33162, Edges: 229735\n",
      "\n",
      " Node types:\n",
      "  paper: [33162]\n",
      "    Edge types: paper-cites->paper\n",
      "\n",
      " Edge types:\n",
      "    paper-cites->paper: [229735]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(G_test.info())\n",
    "print(G_train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "epochs = 100\n",
    "num_samples = [20, 10]\n",
    "\n",
    "train_gen = GraphSAGELinkGenerator(G_train, batch_size, num_samples).flow(\n",
    "    edge_ids_train, edge_labels_train, shuffle=True)\n",
    "test_gen = GraphSAGELinkGenerator(G_test,  batch_size, num_samples).flow(\n",
    "    edge_ids_test, edge_labels_test)\n",
    "\n",
    "layer_sizes = [20, 20]\n",
    "assert len(layer_sizes) == len(num_samples)\n",
    "\n",
    "graphsage = GraphSAGE(\n",
    "        layer_sizes=layer_sizes, generator=train_gen, bias=True, dropout=0.3)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /zhome/ee/2/108654/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "link_classification: using 'ip' method to combine node embeddings into edge embeddings\n"
     ]
    }
   ],
   "source": [
    "# Build the model and expose input and output sockets of graphsage model for link prediction via graphsage.build() method\n",
    "x_inp, x_out = graphsage.build()\n",
    "\n",
    "prediction = link_classification(\n",
    "    output_dim=1, output_act=\"relu\", edge_embedding_method='ip')(x_out)\n",
    "model = keras.Model(inputs=x_inp, outputs=prediction)\n",
    "\n",
    "model.compile(\n",
    "        optimizer=keras.optimizers.Adam(lr=1e-2),\n",
    "        loss=keras.losses.binary_crossentropy,\n",
    "        metrics=[\"acc\"],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:OMP_NUM_THREADS is no longer used by the default Keras config. To configure the number of threads, use tf.config.threading APIs.\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From /zhome/ee/2/108654/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/100\n",
      "2837/2553 - 93s - loss: 0.7587 - acc: 0.5961\n",
      "2553/2553 - 180s - loss: 0.7338 - acc: 0.5712 - val_loss: 0.7761 - val_acc: 0.5961\n",
      "Epoch 2/100\n"
     ]
    }
   ],
   "source": [
    "#init_train_metrics = model.evaluate_generator(train_gen)\n",
    "#init_test_metrics = model.evaluate_generator(test_gen)\n",
    "#\n",
    "#print(\"\\nTrain Set Metrics of the initial (untrained) model:\")\n",
    "#for name, val in zip(model.metrics_names, init_train_metrics):\n",
    "#    print(\"\\t{}: {:0.4f}\".format(name, val))\n",
    "#\n",
    "#print(\"\\nTest Set Metrics of the initial (untrained) model:\")\n",
    "#for name, val in zip(model.metrics_names, init_test_metrics):\n",
    "#    print(\"\\t{}: {:0.4f}\".format(name, val))\n",
    "    \n",
    "history = model.fit_generator(\n",
    "    train_gen,\n",
    "    epochs=epochs,\n",
    "    validation_data=test_gen,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
