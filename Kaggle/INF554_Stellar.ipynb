{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Packages \n",
    "\n",
    "# For graphs\n",
    "import networkx as nx \n",
    "import stellargraph as sg\n",
    "from stellargraph.data import EdgeSplitter\n",
    "from stellargraph.mapper import GraphSAGELinkGenerator\n",
    "from stellargraph.layer import GraphSAGE, HinSAGE, link_classification\n",
    "from stellargraph import globalvar\n",
    "\n",
    "# For DL\n",
    "from tensorflow import keras \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm # progess bar\n",
    "import pickle\n",
    "\n",
    "# For processing node texts\n",
    "import spacy\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.feature_extraction import text as fe\n",
    "\n",
    "# For stemming\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load corpus and ids from pickles\n",
    "corpus_path = r\"pickles/corpus.PICKLE\" \n",
    "ids_path = r\"pickles/IDs.PICKLE\"\n",
    "with open(corpus_path, 'rb') as f:\n",
    "    corpus = pickle.load(f)\n",
    "f.close()\n",
    "with open(ids_path, 'rb') as f:\n",
    "    ids = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# Save in dataframe\n",
    "node_info = pd.DataFrame({'ID': ids, 'Corpus': corpus})\n",
    "node_info_ID = node_info.set_index(['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each set of tokens calculate ratio of each language present\n",
    "def calculate_languages_ratios_from_tokens(tokens):\n",
    "    languages_ratios = []\n",
    "    \n",
    "    # Lower words in set of tokens\n",
    "    words = [word.lower() for word in tokens]\n",
    "    \n",
    "    # Supported languages as intersection\n",
    "    supported_languages = set(stopwords.fileids()) & set(SnowballStemmer.languages)\n",
    "    \n",
    "    # For each language, identify ratio in set of tokens\n",
    "    for language in supported_languages:\n",
    "        stopwords_set = set(stopwords.words(language))\n",
    "        words_set = set(words)\n",
    "        common_elements = words_set.intersection(stopwords_set)\n",
    "        languages_ratios.append(len(common_elements))\n",
    "        \n",
    "    # Set to zero if ratio is zero\n",
    "    if sum(languages_ratios) == 0:\n",
    "        return np.zeros(len(languages_ratios))\n",
    "    \n",
    "    return np.array(languages_ratios)/sum(languages_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stem corpus based on frequent languages\n",
    "\n",
    "# Defining path\n",
    "stemmed_corpus_path = r\"pickles/stemmed_corpus.PICKLE\"\n",
    "\n",
    "\n",
    "if os.path.exists(stemmed_corpus_path):\n",
    "    pass\n",
    "else:\n",
    "    # Supported languages as intersection\n",
    "    supported_languages = list(set(stopwords.fileids()) & set(SnowballStemmer.languages))\n",
    "    stemmed_corpus = []\n",
    "\n",
    "    # For each text in corpus\n",
    "    for text in tqdm(node_info['Corpus'].values, position=0, leave=True):\n",
    "\n",
    "        # Identify tokens\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # Compute language ratios\n",
    "        ratio = calculate_languages_ratios_from_tokens(tokens)\n",
    "\n",
    "        # Note most frequent languages in langs\n",
    "        if np.sum(ratio == 0):\n",
    "            pass\n",
    "        if np.any(ratio>=0.25):\n",
    "            indices = np.where(ratio >= 0.25)[0]\n",
    "            langs = [supported_languages[j] for j in indices]\n",
    "        elif np.all(ratio<0.25) and np.any(ratio>0.10):\n",
    "            indices = np.where(ratio > 0.10)[0]\n",
    "            langs = [supported_languages[j] for j in indices]\n",
    "        else:\n",
    "            langs = [supported_languages[np.argmax(ratio)]]\n",
    "\n",
    "        # For each frequent language stem word if not a stopword \n",
    "        # and if it consists of alphabet letters\n",
    "        for lang in langs:\n",
    "            lang_stopwords = stopwords.words(lang)\n",
    "            stemmer = SnowballStemmer(lang)\n",
    "            tokens = [stemmer.stem(word) for word in tokens if (word not in lang_stopwords) and word.isalpha()]\n",
    "        stemmed_corpus.append(' '.join(tokens))\n",
    "\n",
    "    # Dump pickle\n",
    "    with open(stemmed_corpus_path, '+wb') as f:\n",
    "        pickle.dump(stemmed_corpus, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generating smaller dictionary with stemmed words, no stopwords and frequency > 20\n",
    "\n",
    "# Defining paths\n",
    "small_matrix_path = r\"pickles/small_word_matrix.PICKLE\"\n",
    "corpus_path = r\"pickles/stemmed_corpus.PICKLE\"\n",
    "\n",
    "if os.path.exists(small_matrix_path):\n",
    "    with open(small_matrix_path, 'rb') as f:\n",
    "        word_matrix = pickle.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    with open(corpus_path, 'rb') as f:\n",
    "        corpus = pickle.load(f)\n",
    "        \n",
    "        # Get vectorizer from feature extraction package\n",
    "        vectorizer = fe.CountVectorizer(max_features = 5000, strip_accents = 'unicode')\n",
    "        \n",
    "        # Vectorize corpus\n",
    "        word_matrix = vectorizer.fit_transform(tqdm(corpus))\n",
    "        # Dump pickle\n",
    "        with open(small_matrix_path, '+wb') as g:\n",
    "            pickle.dump(word_matrix, g)\n",
    "        g.close()\n",
    "    f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus and ids path\n",
    "small_matrix_path = r\"pickles/small_word_matrix.PICKLE\"\n",
    "\n",
    "# Open pickle and store word matrix\n",
    "with open(small_matrix_path, 'rb') as f:\n",
    "    small_matrix = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# Creating ids and feature names\n",
    "n1, n2 = small_matrix.shape\n",
    "ids = range(n1)\n",
    "feature_names = [\"w_{}\".format(ii) for ii in range(n2)]\n",
    "\n",
    "# Storing in dataframe\n",
    "node_data = pd.DataFrame(data=small_matrix.toarray(), index=ids, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       w_0  w_1  w_2  w_3  w_4  w_5  w_6  w_7  w_8  w_9  ...  w_4990  w_4991  \\\n",
      "0        0    0    0    4    0    0    0    0    0    0  ...       0       0   \n",
      "1        0    0    0    0    0    0    0    0    0    0  ...       0       0   \n",
      "2        0    0    0    0    0    0    0    0    0    0  ...       0       0   \n",
      "3        0    0    0    0  176    6    0    0    0    0  ...       0       0   \n",
      "4        0    0    0    0    0    0    0    0    0    0  ...       0       0   \n",
      "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...     ...     ...   \n",
      "33221    0    0    0    0    0    0    1    0    0    0  ...       0       0   \n",
      "33222    0    0    0    0    0    0    0    0    0    0  ...       0       0   \n",
      "33223    0    0    0    0    0    1    0    0    0    0  ...       0       0   \n",
      "33224    0    0    0    0    0    0    0    0    0    0  ...       0       0   \n",
      "33225    0    0    0    0    0    0    0    0    0    0  ...       0       0   \n",
      "\n",
      "       w_4992  w_4993  w_4994  w_4995  w_4996  w_4997  w_4998  w_4999  \n",
      "0           0       0       0       0       0       0       0       0  \n",
      "1           0       0       0       0       0       0       0       0  \n",
      "2           0       0       0       0       0       0       0       0  \n",
      "3           0       0       0       0       0       0       0       0  \n",
      "4           0       0       0       0       0       0       0       0  \n",
      "...       ...     ...     ...     ...     ...     ...     ...     ...  \n",
      "33221       0       0       0       0       0       0       0       0  \n",
      "33222       0       0       0       0       0       0       0       0  \n",
      "33223       0       0       0       0       0       0       0       0  \n",
      "33224       0       0       0       0       0       0       0       0  \n",
      "33225       0       0       0       0       0       0       0       0  \n",
      "\n",
      "[33226 rows x 5000 columns]\n"
     ]
    }
   ],
   "source": [
    "print(node_data[feature_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 1000 edges\n",
      "Removed 2000 edges\n",
      "Removed 3000 edges\n",
      "Removed 4000 edges\n",
      "Removed 5000 edges\n",
      "Removed 6000 edges\n",
      "Removed 7000 edges\n",
      "Removed 8000 edges\n",
      "Removed 9000 edges\n",
      "Removed 10000 edges\n",
      "Removed 11000 edges\n",
      "Removed 12000 edges\n",
      "Removed 13000 edges\n",
      "Removed 14000 edges\n",
      "Removed 15000 edges\n",
      "Removed 16000 edges\n",
      "Removed 17000 edges\n",
      "Removed 18000 edges\n",
      "Removed 19000 edges\n",
      "Removed 20000 edges\n",
      "Removed 21000 edges\n",
      "Removed 22000 edges\n",
      "Removed 23000 edges\n",
      "Removed 24000 edges\n",
      "Removed 25000 edges\n",
      "Removed 26000 edges\n",
      "Sampled 1000 negative examples\n",
      "Sampled 2000 negative examples\n",
      "Sampled 3000 negative examples\n",
      "Sampled 4000 negative examples\n",
      "Sampled 5000 negative examples\n",
      "Sampled 6000 negative examples\n",
      "Sampled 7000 negative examples\n",
      "Sampled 8000 negative examples\n",
      "Sampled 9000 negative examples\n",
      "Sampled 10000 negative examples\n",
      "Sampled 11000 negative examples\n",
      "Sampled 12000 negative examples\n",
      "Sampled 13000 negative examples\n",
      "Sampled 14000 negative examples\n",
      "Sampled 15000 negative examples\n",
      "Sampled 16000 negative examples\n",
      "Sampled 17000 negative examples\n",
      "Sampled 18000 negative examples\n",
      "Sampled 19000 negative examples\n",
      "Sampled 20000 negative examples\n",
      "Sampled 21000 negative examples\n",
      "Sampled 22000 negative examples\n",
      "Sampled 23000 negative examples\n",
      "Sampled 24000 negative examples\n",
      "Sampled 25000 negative examples\n",
      "Sampled 26000 negative examples\n",
      "** Sampled 26967 positive and 26967 negative edges. **\n",
      "Removed 1000 edges\n",
      "Removed 2000 edges\n",
      "Removed 3000 edges\n",
      "Removed 4000 edges\n",
      "Removed 5000 edges\n",
      "Removed 6000 edges\n",
      "Removed 7000 edges\n",
      "Removed 8000 edges\n",
      "Removed 9000 edges\n",
      "Removed 10000 edges\n",
      "Removed 11000 edges\n",
      "Removed 12000 edges\n",
      "Removed 13000 edges\n",
      "Removed 14000 edges\n",
      "Removed 15000 edges\n",
      "Removed 16000 edges\n",
      "Removed 17000 edges\n",
      "Removed 18000 edges\n",
      "Removed 19000 edges\n",
      "Removed 20000 edges\n",
      "Removed 21000 edges\n",
      "Removed 22000 edges\n",
      "Removed 23000 edges\n",
      "Removed 24000 edges\n",
      "Sampled 1000 negative examples\n",
      "Sampled 2000 negative examples\n",
      "Sampled 3000 negative examples\n",
      "Sampled 4000 negative examples\n",
      "Sampled 5000 negative examples\n",
      "Sampled 6000 negative examples\n",
      "Sampled 7000 negative examples\n",
      "Sampled 8000 negative examples\n",
      "Sampled 9000 negative examples\n",
      "Sampled 10000 negative examples\n",
      "Sampled 11000 negative examples\n",
      "Sampled 12000 negative examples\n",
      "Sampled 13000 negative examples\n",
      "Sampled 14000 negative examples\n",
      "Sampled 15000 negative examples\n",
      "Sampled 16000 negative examples\n",
      "Sampled 17000 negative examples\n",
      "Sampled 18000 negative examples\n",
      "Sampled 19000 negative examples\n",
      "Sampled 20000 negative examples\n",
      "Sampled 21000 negative examples\n",
      "Sampled 22000 negative examples\n",
      "Sampled 23000 negative examples\n",
      "Sampled 24000 negative examples\n",
      "** Sampled 24270 positive and 24270 negative edges. **\n"
     ]
    }
   ],
   "source": [
    "# Read edges and create NetworkX graph\n",
    "edgelist = pd.read_csv(\"linked_nodes.txt\", sep=' ', header=None, names=[\"source\", \"target\"])\n",
    "edgelist[\"label\"] = \"cites\"  # set the edge type\n",
    "G_all_nx = nx.from_pandas_edgelist(edgelist, edge_attr=\"label\")\n",
    "nx.set_node_attributes(G_all_nx, \"paper\", \"label\")\n",
    "\n",
    "# Initialize Stellargraph with node features of text\n",
    "#G_all = sg.StellarGraph(G_all_nx, node_features=node_data[feature_names])\n",
    "\n",
    "# Define an edge splitter on the original graph G:\n",
    "edge_splitter_test = EdgeSplitter(G_all_nx)\n",
    "\n",
    "# Randomly sample a fraction p=0.1 of all positive links, \n",
    "# and same number of negative links, from G, and obtain the\n",
    "# reduced graph G_test with the sampled links removed:\n",
    "G_test, edge_ids_test, edge_labels_test = edge_splitter_test.train_test_split(\n",
    "    p=0.1, method=\"global\", keep_connected=True)\n",
    "\n",
    "# Define an edge splitter on the reduced graph G_test:\n",
    "edge_splitter_train = EdgeSplitter(G_test)\n",
    "\n",
    "# Randomly sample a fraction p=0.1 of all positive links, and same number of negative links, from G_test, and obtain the\n",
    "# reduced graph G_train with the sampled links removed:\n",
    "G_train, edge_ids_train, edge_labels_train = edge_splitter_train.train_test_split(\n",
    "    p=0.1, method=\"global\", keep_connected=True)\n",
    "\n",
    "G_test = sg.StellarGraph(G_test, node_features=node_data[feature_names])\n",
    "G_train = sg.StellarGraph(G_train, node_features=node_data[feature_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StellarGraph: Undirected multigraph\n",
      " Nodes: 33119, Edges: 242704\n",
      "\n",
      " Node types:\n",
      "  paper: [33119]\n",
      "    Edge types: paper-cites->paper\n",
      "\n",
      " Edge types:\n",
      "    paper-cites->paper: [242704]\n",
      "\n",
      "StellarGraph: Undirected multigraph\n",
      " Nodes: 33119, Edges: 218434\n",
      "\n",
      " Node types:\n",
      "  paper: [33119]\n",
      "    Edge types: paper-cites->paper\n",
      "\n",
      " Edge types:\n",
      "    paper-cites->paper: [218434]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(G_test.info())\n",
    "print(G_train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/aksel/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "epochs = 20\n",
    "num_samples = [10, 5]\n",
    "\n",
    "train_gen = GraphSAGELinkGenerator(G_train, batch_size, num_samples).flow(\n",
    "    edge_ids_train, edge_labels_train, shuffle=True)\n",
    "test_gen = GraphSAGELinkGenerator(G_test,  batch_size, num_samples).flow(\n",
    "    edge_ids_test, edge_labels_test)\n",
    "\n",
    "layer_sizes = [20, 20]\n",
    "assert len(layer_sizes) == len(num_samples)\n",
    "\n",
    "graphsage = GraphSAGE(\n",
    "        layer_sizes=layer_sizes, generator=train_gen, bias=True, dropout=0.3)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "link_classification: using 'ip' method to combine node embeddings into edge embeddings\n"
     ]
    }
   ],
   "source": [
    "# Build the model and expose input and output sockets of graphsage model for link prediction via graphsage.build() method\n",
    "x_inp, x_out = graphsage.build()\n",
    "\n",
    "prediction = link_classification(\n",
    "    output_dim=1, output_act=\"relu\", edge_embedding_method='ip')(x_out)\n",
    "model = keras.Model(inputs=x_inp, outputs=prediction)\n",
    "\n",
    "model.compile(\n",
    "        optimizer=keras.optimizers.Adam(lr=1e-3),\n",
    "        loss=keras.losses.binary_crossentropy,\n",
    "        metrics=[\"acc\"],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:From /Users/aksel/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "2427/2427 - 344s - loss: 0.7332 - acc: 0.5841 - val_loss: 0.7043 - val_acc: 0.5951\n",
      "Epoch 2/20\n",
      "2427/2427 - 327s - loss: 0.6722 - acc: 0.6215 - val_loss: 0.6830 - val_acc: 0.6097\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-734b73d310e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1173\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3275\u001b[0m         \u001b[0mtensor_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3276\u001b[0m         array_vals.append(np.asarray(value,\n\u001b[0;32m-> 3277\u001b[0;31m                                      dtype=tensor_type.as_numpy_dtype))\n\u001b[0m\u001b[1;32m   3278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3279\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \"\"\"\n\u001b[0;32m--> 538\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#init_train_metrics = model.evaluate_generator(train_gen)\n",
    "#init_test_metrics = model.evaluate_generator(test_gen)\n",
    "#\n",
    "#print(\"\\nTrain Set Metrics of the initial (untrained) model:\")\n",
    "#for name, val in zip(model.metrics_names, init_train_metrics):\n",
    "#    print(\"\\t{}: {:0.4f}\".format(name, val))\n",
    "#\n",
    "#print(\"\\nTest Set Metrics of the initial (untrained) model:\")\n",
    "#for name, val in zip(model.metrics_names, init_test_metrics):\n",
    "#    print(\"\\t{}: {:0.4f}\".format(name, val))\n",
    "    \n",
    "history = model.fit_generator(\n",
    "    train_gen,\n",
    "    epochs=epochs,\n",
    "    validation_data=test_gen,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
