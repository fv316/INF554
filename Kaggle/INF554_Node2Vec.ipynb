{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INF554 Kaggle Node2Vec Model\n",
    "#### Francisco, Alex and Aksel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import os\n",
    "import networkx as nx\n",
    "import pdb\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.feature_extraction import text as fe\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import time\n",
    "import keras\n",
    "import lightgbm\n",
    "import spacy\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from hyperopt import STATUS_OK, Trials, hp, space_eval, tpe, fmin\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples shape: (453797, 3)\n",
      "Testing examples shape: (113450, 2)\n"
     ]
    }
   ],
   "source": [
    "with open(r\"training.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    training  = list(reader)\n",
    "# in order of training examples\n",
    "training = [element[0].split(\" \") for element in training]\n",
    "training = pd.DataFrame(training, columns=['Node1', 'Node2', 'Link'])\n",
    "print(\"Training examples shape: {}\".format(training.shape))\n",
    "\n",
    "with open(r\"testing.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    testing  = list(reader)\n",
    "# in order of testing examples\n",
    "testing = [element[0].split(\" \") for element in testing]\n",
    "testing = pd.DataFrame(testing, columns=['Node1', 'Node2'])\n",
    "print(\"Testing examples shape: {}\".format(testing.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(r'pickles'):\n",
    "    os.mkdir(r'pickles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "uncomment lines for reduced corpus with stopword removal. In future integrate stemmer here, multi-language\n",
    "'''\n",
    "NODE_INFO_DIRECTORY = r\"node_information/text/\"\n",
    "\n",
    "corpus_path = r\"pickles/simple_corpus.PICKLE\" \n",
    "ids_path = r\"pickles/ids.PICKLE\"\n",
    "if os.path.exists(corpus_path):\n",
    "    with open(corpus_path, 'rb') as f:\n",
    "        corpus = pickle.load(f)\n",
    "    f.close()\n",
    "    with open(ids_path, 'rb') as f:\n",
    "        ids = pickle.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    corpus = []\n",
    "    ids = []\n",
    "    for filename in tqdm(os.listdir(NODE_INFO_DIRECTORY), position=0, leave=True):\n",
    "        with open(NODE_INFO_DIRECTORY + filename, 'r', encoding='UTF-8', errors='ignore') as f:\n",
    "            doc_string = []\n",
    "            for line in f:\n",
    "                [doc_string.append(token.strip()) for token in line.lower().strip().split(\" \") if token != \"\"]\n",
    "            corpus.append(' '.join(doc_string))\n",
    "            ids.append(filename[:-4])\n",
    "    with open(corpus_path, '+wb') as f:\n",
    "        pickle.dump(corpus, f)\n",
    "    f.close()\n",
    "    with open(ids_path, '+wb') as f:\n",
    "        pickle.dump(ids, f)\n",
    "    f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training node info shape: (33226, 3)\n"
     ]
    }
   ],
   "source": [
    "stemmed_corpus_path = r\"pickles/stemmed_corpus.PICKLE\" \n",
    "if os.path.exists(stemmed_corpus_path):\n",
    "    with open(stemmed_corpus_path, 'rb') as f:\n",
    "        stemmed_corpus = pickle.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    print('Stemmed corpus unavailable')\n",
    "\n",
    "# in order of alphabetical text information i.e. 0, 1, 10, 100\n",
    "node_info = pd.DataFrame({'id': ids, 'corpus': corpus, 'stemmed': stemmed_corpus})\n",
    "node_info_id = node_info.set_index(['id'])\n",
    "print(\"Training node info shape: {}\".format(node_info.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph_split_path = 'pickles/train_graph_split.PICKLE'\n",
    "\n",
    "if os.path.exists(train_graph_split_path):\n",
    "    with open(train_graph_split_path, 'rb') as f:\n",
    "        keep_indices = pickle.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    keep_indices = random.sample(range(len(training)), k=int(len(training) * 0.05))\n",
    "    with open(train_graph_split_path, '+wb') as f:\n",
    "        pickle.dump(keep_indices, f)\n",
    "    f.close()\n",
    "\n",
    "data_train_val = training.iloc[keep_indices]\n",
    "data_train = training.loc[~training.index.isin(keep_indices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_nodes = data_train.loc[data_train['Link']=='1']\n",
    "linked_nodes = linked_nodes[['Node1', 'Node2']]\n",
    "linked_nodes.to_csv('linked_nodes.txt', sep=' ', index=False, header=False)\n",
    "graph=nx.read_edgelist('linked_nodes.txt', create_using=nx.Graph(), nodetype = str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from node2vec import Node2Vec\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node2Vec already calculated\n"
     ]
    }
   ],
   "source": [
    "node2vec_path = r\"pickles/NODE2VEC.PICKLE\"\n",
    "if os.path.exists(node2vec_path):\n",
    "    print('Node2Vec already calculated')\n",
    "else:\n",
    "    x = ''\n",
    "    while (x not in ['n', 'y']):\n",
    "        x = input(\"Node2Vec object not available, recompute? y/n: \")\n",
    "    if x == \"y\":\n",
    "        # Precompute probabilities and generate walks - **ON WINDOWS ONLY WORKS WITH workers=1**\n",
    "        node2vec = Node2Vec(graph, dimensions=64, walk_length=30, num_walks=200, workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "node2vec_model_path = r\"pickles/EMBEDDING_MODEL_FILENAME.model\"\n",
    "if os.path.exists(node2vec_model_path):\n",
    "    model = Word2Vec.load(node2vec_model_path)\n",
    "else:\n",
    "    x = ''\n",
    "    while (x not in ['n', 'y']):\n",
    "        x = input(\"Word2Vec model not available, recompute? y/n: \")\n",
    "    if x == \"y\":\n",
    "        # Embed nodes\n",
    "        model = node2vec.fit(window=10, min_count=1, batch_words=4)  # Any keywords acceptable by gensim.Word2Vec can be passed\n",
    "        # Save embeddings for later use\n",
    "        model.wv.save_word2vec_format('pickles/EMBEDDING_FILENAME.embed')\n",
    "        # Save model for later use\n",
    "        model.save('pickles/EMBEDDING_MODEL_FILENAME.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed edges using Hadamard method\n",
    "from node2vec.edges import HadamardEmbedder\n",
    "edges_embs = HadamardEmbedder(keyed_vectors=model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 22689/22689 [00:54<00:00, 413.47it/s]\n"
     ]
    }
   ],
   "source": [
    "embed_size = len(edges_embs[('0', '1')])\n",
    "df_train = pd.DataFrame(0, index=np.arange(len(data_train_val)), columns=range(embed_size))\n",
    "j = []\n",
    "for j, i in tqdm(enumerate(data_train_val.index), position=0, leave=True, total = len(data_train_val)):\n",
    "    try:\n",
    "        df_train.loc[j] = edges_embs[(data_train_val.loc[i]['Node1'], data_train_val.loc[i]['Node2'])]\n",
    "    except:\n",
    "        df_train.loc[j] = np.zeros(embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 113450/113450 [04:07<00:00, 458.61it/s]\n"
     ]
    }
   ],
   "source": [
    "embed_size = len(edges_embs[('0', '1')])\n",
    "df_test = pd.DataFrame(0, index=np.arange(len(testing)), columns=range(embed_size))\n",
    "j = []\n",
    "for i in tqdm(range(len(testing)), position=0, leave=True):\n",
    "    try:\n",
    "        df_test.loc[i] = edges_embs[(testing.loc[i]['Node1'], testing.loc[i]['Node2'])]\n",
    "    except:\n",
    "        df_test.loc[i] = np.zeros(embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train\n",
    "y = data_train_val['Link']\n",
    "y = list(map(lambda i: int(i), y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9082321187584345\n"
     ]
    }
   ],
   "source": [
    "lgbm = lightgbm.LGBMClassifier()\n",
    "model_lgbm = lgbm.fit(X_train, y_train)\n",
    "predictions = model_lgbm.predict(X_test)\n",
    "print(f1_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = lightgbm.LGBMClassifier()\n",
    "model_lgbm = lgbm.fit(X, y)\n",
    "predictions = model_lgbm.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = pd.concat([pd.DataFrame(np.arange(len(df_test))), pd.DataFrame(predictions)],axis=1)\n",
    "final_pred.columns = ['id', 'predicted']\n",
    "with open(\"node2vec_prediction.csv\",\"+w\") as pred:\n",
    "    csv_out = csv.writer(pred)\n",
    "    csv_out.writerow(['id','predicted'])\n",
    "    for index, row in final_pred.iterrows():\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
