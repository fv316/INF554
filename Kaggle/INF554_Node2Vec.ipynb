{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INF554 Kaggle Node2Vec Model\n",
    "#### Francisco, Alex and Aksel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import os\n",
    "import networkx as nx\n",
    "import pdb\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.feature_extraction import text as fe\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import time\n",
    "import keras\n",
    "import lightgbm\n",
    "import spacy\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from hyperopt import STATUS_OK, Trials, hp, space_eval, tpe, fmin\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples shape: (453797, 3)\n",
      "Testing examples shape: (113450, 2)\n"
     ]
    }
   ],
   "source": [
    "with open(r\"training.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    training  = list(reader)\n",
    "# in order of training examples\n",
    "training = [element[0].split(\" \") for element in training]\n",
    "training = pd.DataFrame(training, columns=['Node1', 'Node2', 'Link'])\n",
    "print(\"Training examples shape: {}\".format(training.shape))\n",
    "\n",
    "with open(r\"testing.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    testing  = list(reader)\n",
    "# in order of testing examples\n",
    "testing = [element[0].split(\" \") for element in testing]\n",
    "testing = pd.DataFrame(testing, columns=['Node1', 'Node2'])\n",
    "print(\"Testing examples shape: {}\".format(testing.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(r'pickles'):\n",
    "    os.mkdir(r'pickles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "uncomment lines for reduced corpus with stopword removal. In future integrate stemmer here, multi-language\n",
    "'''\n",
    "NODE_INFO_DIRECTORY = r\"node_information/text/\"\n",
    "\n",
    "corpus_path = r\"pickles/simple_corpus.PICKLE\" \n",
    "ids_path = r\"pickles/ids.PICKLE\"\n",
    "if os.path.exists(corpus_path):\n",
    "    with open(corpus_path, 'rb') as f:\n",
    "        corpus = pickle.load(f)\n",
    "    f.close()\n",
    "    with open(ids_path, 'rb') as f:\n",
    "        ids = pickle.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    corpus = []\n",
    "    ids = []\n",
    "    for filename in tqdm(os.listdir(NODE_INFO_DIRECTORY), position=0, leave=True):\n",
    "        with open(NODE_INFO_DIRECTORY + filename, 'r', encoding='UTF-8', errors='ignore') as f:\n",
    "            doc_string = []\n",
    "            for line in f:\n",
    "                [doc_string.append(token.strip()) for token in line.lower().strip().split(\" \") if token != \"\"]\n",
    "            corpus.append(' '.join(doc_string))\n",
    "            ids.append(filename[:-4])\n",
    "    with open(corpus_path, '+wb') as f:\n",
    "        pickle.dump(corpus, f)\n",
    "    f.close()\n",
    "    with open(ids_path, '+wb') as f:\n",
    "        pickle.dump(ids, f)\n",
    "    f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training node info shape: (33226, 3)\n"
     ]
    }
   ],
   "source": [
    "stemmed_corpus_path = r\"pickles/stemmed_corpus.PICKLE\" \n",
    "if os.path.exists(stemmed_corpus_path):\n",
    "    with open(stemmed_corpus_path, 'rb') as f:\n",
    "        stemmed_corpus = pickle.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    print('Stemmed corpus unavailable')\n",
    "\n",
    "# in order of alphabetical text information i.e. 0, 1, 10, 100\n",
    "node_info = pd.DataFrame({'id': ids, 'corpus': corpus, 'stemmed': stemmed_corpus})\n",
    "node_info_id = node_info.set_index(['id'])\n",
    "print(\"Training node info shape: {}\".format(node_info.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph_split_path = 'pickles/train_graph_split.PICKLE'\n",
    "\n",
    "if os.path.exists(train_graph_split_path):\n",
    "    with open(train_graph_split_path, 'rb') as f:\n",
    "        keep_indices = pickle.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    keep_indices = random.sample(range(len(training)), k=int(len(training) * 0.05))\n",
    "    with open(train_graph_split_path, '+wb') as f:\n",
    "        pickle.dump(keep_indices, f)\n",
    "    f.close()\n",
    "\n",
    "data_train_val = training.iloc[keep_indices]\n",
    "data_train = training.loc[~training.index.isin(keep_indices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_nodes = data_train.loc[data_train['Link']=='1']\n",
    "linked_nodes = linked_nodes[['Node1', 'Node2']]\n",
    "linked_nodes.to_csv('linked_nodes.txt', sep=' ', index=False, header=False)\n",
    "graph=nx.read_edgelist('linked_nodes.txt', create_using=nx.Graph(), nodetype = str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vector_path = r\"pickles/perf_val_data.PICKLE\"\n",
    "if os.path.exists(feature_vector_path):\n",
    "    with open(feature_vector_path, 'rb') as f:\n",
    "        perf_val_data = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "feature_vector_path = r\"pickles/perf_test_data.PICKLE\"\n",
    "if os.path.exists(feature_vector_path):\n",
    "    with open(feature_vector_path, 'rb') as f:\n",
    "        perf_test_data = pickle.load(f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from node2vec import Node2Vec\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node2Vec already calculated\n"
     ]
    }
   ],
   "source": [
    "node2vec_path = r\"pickles/NODE2VEC.PICKLE\"\n",
    "if os.path.exists(node2vec_path):\n",
    "    print('Node2Vec already calculated')\n",
    "else:\n",
    "    x = ''\n",
    "    while (x not in ['n', 'y']):\n",
    "        x = input(\"Node2Vec object not available, recompute? y/n: \")\n",
    "    if x == \"y\":\n",
    "        # Precompute probabilities and generate walks - **ON WINDOWS ONLY WORKS WITH workers=1**\n",
    "        node2vec = Node2Vec(graph, dimensions=64, walk_length=30, num_walks=200, workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "node2vec_model_path = r\"pickles/EMBEDDING_MODEL_FILENAME.model\"\n",
    "if os.path.exists(node2vec_model_path):\n",
    "    model = Word2Vec.load(node2vec_model_path)\n",
    "else:\n",
    "    x = ''\n",
    "    while (x not in ['n', 'y']):\n",
    "        x = input(\"Word2Vec model not available, recompute? y/n: \")\n",
    "    if x == \"y\":\n",
    "        # Embed nodes\n",
    "        model = node2vec.fit(window=10, min_count=1, batch_words=4)  # Any keywords acceptable by gensim.Word2Vec can be passed\n",
    "        # Save embeddings for later use\n",
    "        model.wv.save_word2vec_format('pickles/EMBEDDING_FILENAME.embed')\n",
    "        # Save model for later use\n",
    "        model.save('pickles/EMBEDDING_MODEL_FILENAME.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed edges using Hadamard method\n",
    "from node2vec.edges import HadamardEmbedder\n",
    "edges_embs = HadamardEmbedder(keyed_vectors=model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 22689/22689 [00:43<00:00, 522.63it/s]\n"
     ]
    }
   ],
   "source": [
    "embed_size = len(edges_embs[('0', '1')])\n",
    "df_train = pd.DataFrame(0, index=np.arange(len(data_train_val)), columns=range(embed_size))\n",
    "j = []\n",
    "for j, i in tqdm(enumerate(data_train_val.index), position=0, leave=True, total = len(data_train_val)):\n",
    "    try:\n",
    "        df_train.loc[j] = edges_embs[(data_train_val.loc[i]['Node1'], data_train_val.loc[i]['Node2'])]\n",
    "    except:\n",
    "        df_train.loc[j] = np.zeros(embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 113450/113450 [03:26<00:00, 549.66it/s]\n"
     ]
    }
   ],
   "source": [
    "embed_size = len(edges_embs[('0', '1')])\n",
    "df_test = pd.DataFrame(0, index=np.arange(len(testing)), columns=range(embed_size))\n",
    "j = []\n",
    "for i in tqdm(range(len(testing)), position=0, leave=True):\n",
    "    try:\n",
    "        df_test.loc[i] = edges_embs[(testing.loc[i]['Node1'], testing.loc[i]['Node2'])]\n",
    "    except:\n",
    "        df_test.loc[i] = np.zeros(embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train\n",
    "y = data_train_val['Link']\n",
    "y = list(map(lambda i: int(i), y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9082321187584345\n"
     ]
    }
   ],
   "source": [
    "lgbm = lightgbm.LGBMClassifier()\n",
    "model_lgbm = lgbm.fit(X_train, y_train)\n",
    "predictions = model_lgbm.predict(X_test)\n",
    "print(f1_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = lightgbm.LGBMClassifier()\n",
    "model_lgbm = lgbm.fit(X, y)\n",
    "predictions = model_lgbm.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = pd.concat([pd.DataFrame(np.arange(len(df_test))), pd.DataFrame(predictions)],axis=1)\n",
    "final_pred.columns = ['id', 'predicted']\n",
    "with open(\"node2vec_prediction.csv\",\"+w\") as pred:\n",
    "    csv_out = csv.writer(pred)\n",
    "    csv_out.writerow(['id','predicted'])\n",
    "    for index, row in final_pred.iterrows():\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting Node2Vec with simple features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Node1</th>\n",
       "      <th>Node2</th>\n",
       "      <th>original_index</th>\n",
       "      <th>tfidf_similarity</th>\n",
       "      <th>languages_similarity</th>\n",
       "      <th>joint_length</th>\n",
       "      <th>file_distance</th>\n",
       "      <th>common_neighbors</th>\n",
       "      <th>jaccard_coefficient</th>\n",
       "      <th>shortest_path</th>\n",
       "      <th>...</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>870</td>\n",
       "      <td>10284</td>\n",
       "      <td>0</td>\n",
       "      <td>0.020013</td>\n",
       "      <td>0.472353</td>\n",
       "      <td>2082371091</td>\n",
       "      <td>9414</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.782416</td>\n",
       "      <td>-11.903319</td>\n",
       "      <td>-2.639962</td>\n",
       "      <td>12.965944</td>\n",
       "      <td>10.962729</td>\n",
       "      <td>-2.729668</td>\n",
       "      <td>-1.796615</td>\n",
       "      <td>-35.957867</td>\n",
       "      <td>-22.477854</td>\n",
       "      <td>1.894107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>620</td>\n",
       "      <td>15300</td>\n",
       "      <td>1</td>\n",
       "      <td>0.302197</td>\n",
       "      <td>0.968905</td>\n",
       "      <td>10964800</td>\n",
       "      <td>14680</td>\n",
       "      <td>12</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1.492524</td>\n",
       "      <td>1.025190</td>\n",
       "      <td>15.935496</td>\n",
       "      <td>4.105417</td>\n",
       "      <td>2.479939</td>\n",
       "      <td>-0.392955</td>\n",
       "      <td>-2.082345</td>\n",
       "      <td>4.230482</td>\n",
       "      <td>-2.446013</td>\n",
       "      <td>3.711744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21115</td>\n",
       "      <td>31904</td>\n",
       "      <td>2</td>\n",
       "      <td>0.005682</td>\n",
       "      <td>0.976566</td>\n",
       "      <td>3102536</td>\n",
       "      <td>10789</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>11.426192</td>\n",
       "      <td>-1.077116</td>\n",
       "      <td>-3.555380</td>\n",
       "      <td>0.343145</td>\n",
       "      <td>4.553902</td>\n",
       "      <td>2.437573</td>\n",
       "      <td>13.295860</td>\n",
       "      <td>-0.094963</td>\n",
       "      <td>3.361268</td>\n",
       "      <td>-9.936666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3021</td>\n",
       "      <td>28396</td>\n",
       "      <td>3</td>\n",
       "      <td>0.001427</td>\n",
       "      <td>0.438494</td>\n",
       "      <td>26860638</td>\n",
       "      <td>25375</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.053369</td>\n",
       "      <td>3.163022</td>\n",
       "      <td>-1.477589</td>\n",
       "      <td>-2.987408</td>\n",
       "      <td>-0.804287</td>\n",
       "      <td>-0.181976</td>\n",
       "      <td>-10.356342</td>\n",
       "      <td>-7.429958</td>\n",
       "      <td>-8.883316</td>\n",
       "      <td>-7.870898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10780</td>\n",
       "      <td>6135</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035456</td>\n",
       "      <td>0.524943</td>\n",
       "      <td>501323778</td>\n",
       "      <td>4645</td>\n",
       "      <td>8</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032880</td>\n",
       "      <td>8.877627</td>\n",
       "      <td>3.435661</td>\n",
       "      <td>6.144619</td>\n",
       "      <td>15.207212</td>\n",
       "      <td>-0.159521</td>\n",
       "      <td>0.084702</td>\n",
       "      <td>4.289405</td>\n",
       "      <td>0.027232</td>\n",
       "      <td>13.226315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113445</th>\n",
       "      <td>13419</td>\n",
       "      <td>13751</td>\n",
       "      <td>113445</td>\n",
       "      <td>0.038824</td>\n",
       "      <td>0.992118</td>\n",
       "      <td>1483618868</td>\n",
       "      <td>332</td>\n",
       "      <td>1</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.959877</td>\n",
       "      <td>9.896625</td>\n",
       "      <td>-0.015895</td>\n",
       "      <td>-4.225902</td>\n",
       "      <td>14.681097</td>\n",
       "      <td>1.192195</td>\n",
       "      <td>16.902082</td>\n",
       "      <td>20.310263</td>\n",
       "      <td>0.930499</td>\n",
       "      <td>0.162470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113446</th>\n",
       "      <td>16696</td>\n",
       "      <td>20191</td>\n",
       "      <td>113446</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>319676</td>\n",
       "      <td>3495</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>4.652687</td>\n",
       "      <td>-0.041763</td>\n",
       "      <td>8.305681</td>\n",
       "      <td>-0.363197</td>\n",
       "      <td>1.729668</td>\n",
       "      <td>1.210054</td>\n",
       "      <td>-0.814669</td>\n",
       "      <td>1.022599</td>\n",
       "      <td>0.554587</td>\n",
       "      <td>14.763882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113447</th>\n",
       "      <td>10654</td>\n",
       "      <td>27692</td>\n",
       "      <td>113447</td>\n",
       "      <td>0.002498</td>\n",
       "      <td>0.269025</td>\n",
       "      <td>62737199</td>\n",
       "      <td>17038</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.571234</td>\n",
       "      <td>-16.742168</td>\n",
       "      <td>11.482271</td>\n",
       "      <td>-2.861252</td>\n",
       "      <td>-0.111475</td>\n",
       "      <td>1.638025</td>\n",
       "      <td>2.307337</td>\n",
       "      <td>3.496703</td>\n",
       "      <td>-6.055089</td>\n",
       "      <td>-9.927100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113448</th>\n",
       "      <td>5409</td>\n",
       "      <td>1668</td>\n",
       "      <td>113448</td>\n",
       "      <td>0.071846</td>\n",
       "      <td>0.992244</td>\n",
       "      <td>90309525</td>\n",
       "      <td>3741</td>\n",
       "      <td>6</td>\n",
       "      <td>0.027650</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.902737</td>\n",
       "      <td>-35.269802</td>\n",
       "      <td>10.885975</td>\n",
       "      <td>45.663902</td>\n",
       "      <td>-53.291111</td>\n",
       "      <td>-4.106681</td>\n",
       "      <td>43.381569</td>\n",
       "      <td>-45.706051</td>\n",
       "      <td>-2.498136</td>\n",
       "      <td>5.635628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113449</th>\n",
       "      <td>1789</td>\n",
       "      <td>996</td>\n",
       "      <td>113449</td>\n",
       "      <td>0.017850</td>\n",
       "      <td>0.949696</td>\n",
       "      <td>1769704583</td>\n",
       "      <td>793</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>9.689781</td>\n",
       "      <td>3.142098</td>\n",
       "      <td>-3.322826</td>\n",
       "      <td>2.212464</td>\n",
       "      <td>2.666095</td>\n",
       "      <td>-0.498620</td>\n",
       "      <td>-5.664875</td>\n",
       "      <td>-27.038015</td>\n",
       "      <td>17.063435</td>\n",
       "      <td>1.285801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>113450 rows × 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Node1  Node2  original_index  tfidf_similarity  languages_similarity  \\\n",
       "0         870  10284               0          0.020013              0.472353   \n",
       "1         620  15300               1          0.302197              0.968905   \n",
       "2       21115  31904               2          0.005682              0.976566   \n",
       "3        3021  28396               3          0.001427              0.438494   \n",
       "4       10780   6135               4          0.035456              0.524943   \n",
       "...       ...    ...             ...               ...                   ...   \n",
       "113445  13419  13751          113445          0.038824              0.992118   \n",
       "113446  16696  20191          113446          0.000000              0.000000   \n",
       "113447  10654  27692          113447          0.002498              0.269025   \n",
       "113448   5409   1668          113448          0.071846              0.992244   \n",
       "113449   1789    996          113449          0.017850              0.949696   \n",
       "\n",
       "        joint_length  file_distance  common_neighbors  jaccard_coefficient  \\\n",
       "0         2082371091           9414                 0             0.000000   \n",
       "1           10964800          14680                12             0.044444   \n",
       "2            3102536          10789                 0             0.000000   \n",
       "3           26860638          25375                 0             0.000000   \n",
       "4          501323778           4645                 8             0.177778   \n",
       "...              ...            ...               ...                  ...   \n",
       "113445    1483618868            332                 1             0.071429   \n",
       "113446        319676           3495                 0             0.000000   \n",
       "113447      62737199          17038                 0             0.000000   \n",
       "113448      90309525           3741                 6             0.027650   \n",
       "113449    1769704583            793                 1             0.010753   \n",
       "\n",
       "        shortest_path  ...         54         55         56         57  \\\n",
       "0                   4  ...   0.782416 -11.903319  -2.639962  12.965944   \n",
       "1                   3  ...   1.492524   1.025190  15.935496   4.105417   \n",
       "2                   4  ...  11.426192  -1.077116  -3.555380   0.343145   \n",
       "3                   4  ... -11.053369   3.163022  -1.477589  -2.987408   \n",
       "4                   3  ...  -0.032880   8.877627   3.435661   6.144619   \n",
       "...               ...  ...        ...        ...        ...        ...   \n",
       "113445              3  ...   0.959877   9.896625  -0.015895  -4.225902   \n",
       "113446              5  ...   4.652687  -0.041763   8.305681  -0.363197   \n",
       "113447              4  ...  -0.571234 -16.742168  11.482271  -2.861252   \n",
       "113448              3  ...  -1.902737 -35.269802  10.885975  45.663902   \n",
       "113449              3  ...   9.689781   3.142098  -3.322826   2.212464   \n",
       "\n",
       "               58        59         60         61         62         63  \n",
       "0       10.962729 -2.729668  -1.796615 -35.957867 -22.477854   1.894107  \n",
       "1        2.479939 -0.392955  -2.082345   4.230482  -2.446013   3.711744  \n",
       "2        4.553902  2.437573  13.295860  -0.094963   3.361268  -9.936666  \n",
       "3       -0.804287 -0.181976 -10.356342  -7.429958  -8.883316  -7.870898  \n",
       "4       15.207212 -0.159521   0.084702   4.289405   0.027232  13.226315  \n",
       "...           ...       ...        ...        ...        ...        ...  \n",
       "113445  14.681097  1.192195  16.902082  20.310263   0.930499   0.162470  \n",
       "113446   1.729668  1.210054  -0.814669   1.022599   0.554587  14.763882  \n",
       "113447  -0.111475  1.638025   2.307337   3.496703  -6.055089  -9.927100  \n",
       "113448 -53.291111 -4.106681  43.381569 -45.706051  -2.498136   5.635628  \n",
       "113449   2.666095 -0.498620  -5.664875 -27.038015  17.063435   1.285801  \n",
       "\n",
       "[113450 rows x 83 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extended_model_train = pd.concat([perf_val_data, df_train], axis =1)\n",
    "extended_model_test = pd.concat([perf_test_data, df_test], axis =1)\n",
    "extended_model_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_columns = ['original_index', 'Link', 'Node1', 'Node2']\n",
    "cols = [i for i in extended_model_train.columns if i not in d_columns]\n",
    "X = extended_model_train[cols]\n",
    "\n",
    "y = data_train_val['Link']\n",
    "y = list(map(lambda i: int(i), y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9487821534259048\n"
     ]
    }
   ],
   "source": [
    "lgbm = lightgbm.LGBMClassifier()\n",
    "model_lgbm = lgbm.fit(X_train, y_train)\n",
    "predictions = model_lgbm.predict(X_test)\n",
    "print(f1_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = lightgbm.LGBMClassifier()\n",
    "model_lgbm = lgbm.fit(X, y)\n",
    "predictions = model_lgbm.predict(extended_model_test[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = pd.concat([pd.DataFrame(np.arange(len(df_test))), pd.DataFrame(predictions)],axis=1)\n",
    "final_pred.columns = ['id', 'predicted']\n",
    "with open(\"model_ensemble_prediction.csv\",\"+w\") as pred:\n",
    "    csv_out = csv.writer(pred)\n",
    "    csv_out.writerow(['id','predicted'])\n",
    "    for index, row in final_pred.iterrows():\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9423296102119899\n"
     ]
    }
   ],
   "source": [
    "d_columns = ['original_index', 'Link', 'Node1', 'Node2']\n",
    "cols = [i for i in perf_val_data.columns if i not in d_columns]\n",
    "X = perf_val_data[cols]\n",
    "y = data_train_val['Link']\n",
    "y = list(map(lambda i: int(i), y))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=1)\n",
    "lgbm = lightgbm.LGBMClassifier()\n",
    "model_lgbm = lgbm.fit(X_train, y_train)\n",
    "predictions = model_lgbm.predict(X_test)\n",
    "print(f1_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = lightgbm.LGBMClassifier()\n",
    "model_lgbm = lgbm.fit(X, y)\n",
    "predictions = model_lgbm.predict(extended_model_test[cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating simple ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_features_predictions = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "node2vec_predictions = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_predictions = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = np.array([int(i) for i in ((simple_predictions + node2vec_predictions + extended_features_predictions) >= 2)])\n",
    "predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113450"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9482523831139356\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99869"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(simple_predictions == node2vec_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102331"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(extended_features_predictions == node2vec_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109126"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(simple_predictions == extended_features_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "node2vec_path = r\"pickles/NODE2VEC.PICKLE\"\n",
    "if os.path.exists(node2vec_path):\n",
    "    with open(node2vec_path, 'rb') as f:\n",
    "        node2vec = pickle.load(f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 22689/22689 [00:49<00:00, 456.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9111969111969113\n"
     ]
    }
   ],
   "source": [
    "model = node2vec.fit(window=20, min_count=25, batch_words=10)  # Any keywords acceptable by gensim.Word2Vec can be passed\n",
    "# Embed edges using Hadamard method\n",
    "from node2vec.edges import HadamardEmbedder\n",
    "edges_embs = HadamardEmbedder(keyed_vectors=model.wv)\n",
    "\n",
    "embed_size = len(edges_embs[('0', '1')])\n",
    "df_train = pd.DataFrame(0, index=np.arange(len(data_train_val)), columns=range(embed_size))\n",
    "j = []\n",
    "for j, i in tqdm(enumerate(data_train_val.index), position=0, leave=True, total = len(data_train_val)):\n",
    "    try:\n",
    "        df_train.loc[j] = edges_embs[(data_train_val.loc[i]['Node1'], data_train_val.loc[i]['Node2'])]\n",
    "    except:\n",
    "        df_train.loc[j] = np.zeros(embed_size)\n",
    "X = df_train\n",
    "y = data_train_val['Link']\n",
    "y = list(map(lambda i: int(i), y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=1)\n",
    "lgbm = lightgbm.LGBMClassifier()\n",
    "model_lgbm = lgbm.fit(X_train, y_train)\n",
    "predictions = model_lgbm.predict(X_test)\n",
    "print(f1_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 22689/22689 [01:06<00:00, 339.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.909952606635071\n"
     ]
    }
   ],
   "source": [
    "model = node2vec.fit(window=5, min_count=20, batch_words=10)  # Any keywords acceptable by gensim.Word2Vec can be passed\n",
    "# Embed edges using Hadamard method\n",
    "from node2vec.edges import HadamardEmbedder\n",
    "edges_embs = HadamardEmbedder(keyed_vectors=model.wv)\n",
    "\n",
    "embed_size = len(edges_embs[('0', '1')])\n",
    "df_train = pd.DataFrame(0, index=np.arange(len(data_train_val)), columns=range(embed_size))\n",
    "j = []\n",
    "for j, i in tqdm(enumerate(data_train_val.index), position=0, leave=True, total = len(data_train_val)):\n",
    "    try:\n",
    "        df_train.loc[j] = edges_embs[(data_train_val.loc[i]['Node1'], data_train_val.loc[i]['Node2'])]\n",
    "    except:\n",
    "        df_train.loc[j] = np.zeros(embed_size)\n",
    "X = df_train\n",
    "y = data_train_val['Link']\n",
    "y = list(map(lambda i: int(i), y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=1)\n",
    "lgbm = lightgbm.LGBMClassifier()\n",
    "model_lgbm = lgbm.fit(X_train, y_train)\n",
    "predictions = model_lgbm.predict(X_test)\n",
    "print(f1_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 22689/22689 [00:46<00:00, 489.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9146204100022527\n"
     ]
    }
   ],
   "source": [
    "model = node2vec.fit(window=10, min_count=40, batch_words=4)  # Any keywords acceptable by gensim.Word2Vec can be passed\n",
    "# Embed edges using Hadamard method\n",
    "from node2vec.edges import HadamardEmbedder\n",
    "edges_embs = HadamardEmbedder(keyed_vectors=model.wv)\n",
    "\n",
    "embed_size = len(edges_embs[('0', '1')])\n",
    "df_train = pd.DataFrame(0, index=np.arange(len(data_train_val)), columns=range(embed_size))\n",
    "j = []\n",
    "for j, i in tqdm(enumerate(data_train_val.index), position=0, leave=True, total = len(data_train_val)):\n",
    "    try:\n",
    "        df_train.loc[j] = edges_embs[(data_train_val.loc[i]['Node1'], data_train_val.loc[i]['Node2'])]\n",
    "    except:\n",
    "        df_train.loc[j] = np.zeros(embed_size)\n",
    "X = df_train\n",
    "y = data_train_val['Link']\n",
    "y = list(map(lambda i: int(i), y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=1)\n",
    "lgbm = lightgbm.LGBMClassifier()\n",
    "model_lgbm = lgbm.fit(X_train, y_train)\n",
    "predictions = model_lgbm.predict(X_test)\n",
    "print(f1_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 22689/22689 [00:59<00:00, 382.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.909952606635071\n"
     ]
    }
   ],
   "source": [
    "model = node2vec.fit(window=5, min_count=40, batch_words=4)  # Any keywords acceptable by gensim.Word2Vec can be passed\n",
    "# Embed edges using Hadamard method\n",
    "from node2vec.edges import HadamardEmbedder\n",
    "edges_embs = HadamardEmbedder(keyed_vectors=model.wv)\n",
    "\n",
    "embed_size = len(edges_embs[('0', '1')])\n",
    "df_train = pd.DataFrame(0, index=np.arange(len(data_train_val)), columns=range(embed_size))\n",
    "j = []\n",
    "for j, i in tqdm(enumerate(data_train_val.index), position=0, leave=True, total = len(data_train_val)):\n",
    "    try:\n",
    "        df_train.loc[j] = edges_embs[(data_train_val.loc[i]['Node1'], data_train_val.loc[i]['Node2'])]\n",
    "    except:\n",
    "        df_train.loc[j] = np.zeros(embed_size)\n",
    "X = df_train\n",
    "y = data_train_val['Link']\n",
    "y = list(map(lambda i: int(i), y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=1)\n",
    "lgbm = lightgbm.LGBMClassifier()\n",
    "model_lgbm = lgbm.fit(X_train, y_train)\n",
    "predictions = model_lgbm.predict(X_test)\n",
    "print(f1_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 22689/22689 [00:52<00:00, 433.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9010592742844264\n"
     ]
    }
   ],
   "source": [
    "model = node2vec.fit(window=40, min_count=40, batch_words=4)  # Any keywords acceptable by gensim.Word2Vec can be passed\n",
    "# Embed edges using Hadamard method\n",
    "from node2vec.edges import HadamardEmbedder\n",
    "edges_embs = HadamardEmbedder(keyed_vectors=model.wv)\n",
    "\n",
    "embed_size = len(edges_embs[('0', '1')])\n",
    "df_train = pd.DataFrame(0, index=np.arange(len(data_train_val)), columns=range(embed_size))\n",
    "j = []\n",
    "for j, i in tqdm(enumerate(data_train_val.index), position=0, leave=True, total = len(data_train_val)):\n",
    "    try:\n",
    "        df_train.loc[j] = edges_embs[(data_train_val.loc[i]['Node1'], data_train_val.loc[i]['Node2'])]\n",
    "    except:\n",
    "        df_train.loc[j] = np.zeros(embed_size)\n",
    "X = df_train\n",
    "y = data_train_val['Link']\n",
    "y = list(map(lambda i: int(i), y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=1)\n",
    "lgbm = lightgbm.LGBMClassifier()\n",
    "model_lgbm = lgbm.fit(X_train, y_train)\n",
    "predictions = model_lgbm.predict(X_test)\n",
    "print(f1_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = node2vec.fit(window=10, min_count=40, batch_words=2)  # Any keywords acceptable by gensim.Word2Vec can be passed\n",
    "# Embed edges using Hadamard method\n",
    "from node2vec.edges import HadamardEmbedder\n",
    "edges_embs = HadamardEmbedder(keyed_vectors=model.wv)\n",
    "\n",
    "embed_size = len(edges_embs[('0', '1')])\n",
    "df_train = pd.DataFrame(0, index=np.arange(len(data_train_val)), columns=range(embed_size))\n",
    "j = []\n",
    "for j, i in tqdm(enumerate(data_train_val.index), position=0, leave=True, total = len(data_train_val)):\n",
    "    try:\n",
    "        df_train.loc[j] = edges_embs[(data_train_val.loc[i]['Node1'], data_train_val.loc[i]['Node2'])]\n",
    "    except:\n",
    "        df_train.loc[j] = np.zeros(embed_size)\n",
    "X = df_train\n",
    "y = data_train_val['Link']\n",
    "y = list(map(lambda i: int(i), y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=1)\n",
    "lgbm = lightgbm.LGBMClassifier()\n",
    "model_lgbm = lgbm.fit(X_train, y_train)\n",
    "predictions = model_lgbm.predict(X_test)\n",
    "print(f1_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = node2vec.fit(window=10, min_count=40, batch_words=1)  # Any keywords acceptable by gensim.Word2Vec can be passed\n",
    "# Embed edges using Hadamard method\n",
    "from node2vec.edges import HadamardEmbedder\n",
    "edges_embs = HadamardEmbedder(keyed_vectors=model.wv)\n",
    "\n",
    "embed_size = len(edges_embs[('0', '1')])\n",
    "df_train = pd.DataFrame(0, index=np.arange(len(data_train_val)), columns=range(embed_size))\n",
    "j = []\n",
    "for j, i in tqdm(enumerate(data_train_val.index), position=0, leave=True, total = len(data_train_val)):\n",
    "    try:\n",
    "        df_train.loc[j] = edges_embs[(data_train_val.loc[i]['Node1'], data_train_val.loc[i]['Node2'])]\n",
    "    except:\n",
    "        df_train.loc[j] = np.zeros(embed_size)\n",
    "X = df_train\n",
    "y = data_train_val['Link']\n",
    "y = list(map(lambda i: int(i), y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=1)\n",
    "lgbm = lightgbm.LGBMClassifier()\n",
    "model_lgbm = lgbm.fit(X_train, y_train)\n",
    "predictions = model_lgbm.predict(X_test)\n",
    "print(f1_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
