{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Packages \n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "# For graphs\n",
    "import networkx as nx \n",
    "import stellargraph as sg\n",
    "from stellargraph.data import EdgeSplitter\n",
    "from stellargraph.mapper import GraphSAGELinkGenerator\n",
    "from stellargraph.layer import GraphSAGE, HinSAGE, link_classification\n",
    "from stellargraph import globalvar\n",
    "\n",
    "from tensorflow import keras \n",
    "import tensorflow as tf\n",
    "\n",
    "'''\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=4, inter_op_parallelism_threads=4, allow_soft_placement=True, device_count = {'CPU': 4})\n",
    "session = tf.Session(config=config)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"4\"\n",
    "os.environ[\"KMP_BLOCKTIME\"] = \"30\"\n",
    "os.environ[\"KMP_SETTINGS\"] = \"1\"\n",
    "os.environ[\"KMP_AFFINITY\"]= \"granularity=fine,verbose,compact,1,0\"\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "# For processing node texts\n",
    "from sklearn.feature_extraction import text as fe\n",
    "\n",
    "# Dimensionality reduction\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "# Word embeddings\n",
    "import gensim \n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# For stemming\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "tf.test.gpu_device_name()\n",
    "#!cat /proc/meminfo\n",
    "!cat /proc/cpuinfo\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load corpus and ids from pickles\n",
    "corpus_path = r\"pickles/simple_corpus.PICKLE\" \n",
    "ids_path = r\"pickles/IDs.PICKLE\"\n",
    "with open(corpus_path, 'rb') as f:\n",
    "    corpus = pickle.load(f)\n",
    "f.close()\n",
    "with open(ids_path, 'rb') as f:\n",
    "    ids = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# Save in dataframe\n",
    "node_info = pd.DataFrame({'ID': ids, 'Corpus': corpus})\n",
    "node_info_ID = node_info.set_index(['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each set of tokens calculate ratio of each language present\n",
    "def calculate_languages_ratios_from_tokens(tokens):\n",
    "    languages_ratios = []\n",
    "    \n",
    "    # Lower words in set of tokens\n",
    "    words = [word.lower() for word in tokens]\n",
    "    \n",
    "    # Supported languages as intersection\n",
    "    supported_languages = set(stopwords.fileids()) & set(SnowballStemmer.languages)\n",
    "    \n",
    "    # For each language, identify ratio in set of tokens\n",
    "    for language in supported_languages:\n",
    "        stopwords_set = set(stopwords.words(language))\n",
    "        words_set = set(words)\n",
    "        common_elements = words_set.intersection(stopwords_set)\n",
    "        languages_ratios.append(len(common_elements))\n",
    "        \n",
    "    # Set to zero if ratio is zero\n",
    "    if sum(languages_ratios) == 0:\n",
    "        return np.zeros(len(languages_ratios))\n",
    "    \n",
    "    return np.array(languages_ratios)/sum(languages_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stem corpus based on frequent languages\n",
    "\n",
    "# Defining path\n",
    "stemmed_corpus_path = r\"pickles/stemmed_corpus.PICKLE\"\n",
    "\n",
    "\n",
    "if os.path.exists(stemmed_corpus_path):\n",
    "    pass\n",
    "else:\n",
    "    # Supported languages as intersection\n",
    "    supported_languages = list(set(stopwords.fileids()) & set(SnowballStemmer.languages))\n",
    "    stemmed_corpus = []\n",
    "\n",
    "    # For each text in corpus\n",
    "    for text in tqdm(node_info['Corpus'].values, position=0, leave=True):\n",
    "\n",
    "        # Identify tokens\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # Compute language ratios\n",
    "        ratio = calculate_languages_ratios_from_tokens(tokens)\n",
    "\n",
    "        # Note most frequent languages in langs\n",
    "        if np.sum(ratio == 0):\n",
    "            pass\n",
    "        if np.any(ratio>=0.25):\n",
    "            indices = np.where(ratio >= 0.25)[0]\n",
    "            langs = [supported_languages[j] for j in indices]\n",
    "        elif np.all(ratio<0.25) and np.any(ratio>0.10):\n",
    "            indices = np.where(ratio > 0.10)[0]\n",
    "            langs = [supported_languages[j] for j in indices]\n",
    "        else:\n",
    "            langs = [supported_languages[np.argmax(ratio)]]\n",
    "\n",
    "        # For each frequent language stem word if not a stopword \n",
    "        # and if it consists of alphabet letters\n",
    "        for lang in langs:\n",
    "            lang_stopwords = stopwords.words(lang)\n",
    "            stemmer = SnowballStemmer(lang)\n",
    "            tokens = [stemmer.stem(word) for word in tokens if (word not in lang_stopwords) and word.isalpha()]\n",
    "        stemmed_corpus.append(' '.join(tokens))\n",
    "\n",
    "    # Dump pickle\n",
    "    with open(stemmed_corpus_path, '+wb') as f:\n",
    "        pickle.dump(stemmed_corpus, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generating smaller dictionary with stemmed words, no stopwords and frequency > 20\n",
    "\n",
    "# Defining paths\n",
    "small_matrix_path = r\"pickles/small_word_matrix.PICKLE\"\n",
    "corpus_path = r\"pickles/stemmed_corpus.PICKLE\"\n",
    "\n",
    "if os.path.exists(small_matrix_path):\n",
    "    with open(small_matrix_path, 'rb') as f:\n",
    "        word_matrix = pickle.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    with open(corpus_path, 'rb') as f:\n",
    "        stemmed_corpus = pickle.load(f)\n",
    "        \n",
    "        # Get vectorizer from feature extraction package\n",
    "        vectorizer = fe.CountVectorizer(min_df = 20, max_df = 0.9, strip_accents = 'unicode')\n",
    "        \n",
    "        # Vectorize corpus\n",
    "        word_matrix = vectorizer.fit_transform(tqdm(stemmed_corpus))\n",
    "        \n",
    "        # Dump pickle\n",
    "        with open(small_matrix_path, '+wb') as g:\n",
    "            pickle.dump(word_matrix, g)\n",
    "        g.close()\n",
    "    f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_matrix_path = r\"pickles/corpus_tfidf_matrix.PICKLE\"\n",
    "if os.path.exists(corpus_matrix_path):\n",
    "    with open(corpus_matrix_path, 'rb') as f:\n",
    "        corpus_tfidf_matrix = pickle.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    with open(corpus_path, 'rb') as g:\n",
    "        stemmed_corpus = pickle.load(g)\n",
    "        vectorizer3 = fe.TfidfVectorizer(min_df = 20, max_df = 0.9, strip_accents = 'unicode')\n",
    "        corpus_tfidf_matrix = vectorizer3.fit_transform(tqdm(stemmed_corpus))\n",
    "        with open(corpus_matrix_path, '+wb') as f:\n",
    "            pickle.dump(corpus_tfidf_matrix, f)\n",
    "        f.close()\n",
    "    g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = corpus_tfidf_matrix.shape[1]\n",
    "n_components = 100\n",
    "\n",
    "nmf_frobenius_path = r\"pickles/nmf_frobenius_matrix.PICKLE\"\n",
    "if os.path.exists(nmf_frobenius_path):\n",
    "    with open(nmf_frobenius_path, 'rb') as f:\n",
    "        nmf_frobenius = pickle.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
    "          \"n_samples=%d and n_features=%d...\" % (corpus_tfidf_matrix.shape[0], n_features))\n",
    "\n",
    "    nmf_frobenius = NMF(n_components=n_components, random_state=1, alpha=.1, l1_ratio=.5).fit(corpus_tfidf_matrix)\n",
    "    with open(nmf_frobenius_path, '+wb') as f:\n",
    "        pickle.dump(nmf_frobenius, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_reduced_matrix_path = r\"pickles/nmf_frobenius_matrix_realized.PICKLE\"\n",
    "if os.path.exists(small_reduced_matrix_path):\n",
    "    with open(small_reduced_matrix_path, 'rb') as f:\n",
    "        small_reduced_matrix = pickle.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    with open(small_reduced_matrix_path, '+wb') as f:\n",
    "        small_reduced_matrix = nmf_frobenius.fit_transform(corpus_tfidf_matrix)\n",
    "        pickle.dump(small_reduced_matrix, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA models with tf features, n_samples=33226 and n_features=42817...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got scalar array instead:\narray=0it [00:00, ?it/s].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-c723d8366a99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m                                     random_state=0)\n\u001b[1;32m     18\u001b[0m     \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mlda_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'+wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/decomposition/online_lda.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \"\"\"\n\u001b[1;32m    530\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_non_neg_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"LatentDirichletAllocation.fit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0mmax_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/decomposition/online_lda.py\u001b[0m in \u001b[0;36m_check_non_neg_array\u001b[0;34m(self, X, whom)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \"\"\"\n\u001b[0;32m--> 468\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m         \u001b[0mcheck_non_negative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    512\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    515\u001b[0m             \u001b[0;31m# If input is 1D raise error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got scalar array instead:\narray=0it [00:00, ?it/s].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "lda_path = r\"pickles/lda_matrix.PICKLE\"\n",
    "\n",
    "\n",
    "if os.path.exists(lda_path):\n",
    "    with open(lda_path, 'rb') as f:\n",
    "        lda_matrix = pickle.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    n_features = word_matrix.shape[1]\n",
    "    n_components = 100\n",
    "    print(\"Fitting LDA models with tf features, \"\n",
    "          \"n_samples=%d and n_features=%d...\"\n",
    "          % (word_matrix.shape[0], n_features))\n",
    "    lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                    learning_method='online',\n",
    "                                    learning_offset=50.,\n",
    "                                    random_state=0)\n",
    "    lda.fit(word_matrix)\n",
    "    lda_matrix = lda.fit_transform(word_matrix)\n",
    "    with open(lda_path, '+wb') as f:\n",
    "        pickle.dump(lda_matrix, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_matrix = lda.fit_transform(word_matrix)\n",
    "with open(lda_path, '+wb') as f:\n",
    "    pickle.dump(lda_matrix, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33226 100\n"
     ]
    }
   ],
   "source": [
    "# Storing in dataframe\n",
    "n1, n2 = lda_matrix.shape\n",
    "# Creating feature names\n",
    "feature_names = [\"w_{}\".format(ii) for ii in range(n2)]\n",
    "ids = sorted(range(n1), key=str)\n",
    "node_data = pd.DataFrame(data=lda_matrix, index=ids, columns=feature_names)\n",
    "print(n1,n2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               w_0       w_1       w_2           w_3       w_4       w_5  \\\n",
      "0     2.898551e-05  0.000029  0.000029  2.898551e-05  0.000029  0.000029   \n",
      "1     5.168863e-03  0.000005  0.063812  4.659832e-06  0.000005  0.005558   \n",
      "10    6.618134e-06  0.000007  0.050432  6.618134e-06  0.000007  0.000007   \n",
      "100   6.193101e-07  0.137591  0.021816  6.193101e-07  0.000684  0.013460   \n",
      "1000  3.003003e-05  0.004844  0.000030  3.003003e-05  0.000030  0.000030   \n",
      "...            ...       ...       ...           ...       ...       ...   \n",
      "9995  4.975124e-05  0.000050  0.000050  4.975124e-05  0.010127  0.042436   \n",
      "9996  5.356186e-06  0.000005  0.000005  5.356186e-06  0.000005  0.000005   \n",
      "9997  5.588217e-02  0.000003  0.006002  3.004808e-06  0.000003  0.206994   \n",
      "9998  5.820722e-06  0.036649  0.063503  5.820722e-06  0.000006  0.000006   \n",
      "9999  5.263158e-04  0.000526  0.000526  5.263158e-04  0.198031  0.000526   \n",
      "\n",
      "           w_6           w_7           w_8       w_9  ...      w_90      w_91  \\\n",
      "0     0.000029  9.288738e-01  2.898551e-05  0.000029  ...  0.000029  0.000029   \n",
      "1     0.000005  4.659832e-06  4.659832e-06  0.000005  ...  0.000005  0.667795   \n",
      "10    0.000007  6.618134e-06  6.618134e-06  0.000007  ...  0.002203  0.036878   \n",
      "100   0.000137  6.193101e-07  6.193101e-07  0.150384  ...  0.001708  0.020330   \n",
      "1000  0.000030  3.003003e-05  3.003003e-05  0.000030  ...  0.020967  0.026147   \n",
      "...        ...           ...           ...       ...  ...       ...       ...   \n",
      "9995  0.010081  4.975124e-05  4.975124e-05  0.000050  ...  0.000050  0.000050   \n",
      "9996  0.000005  5.356186e-06  5.356186e-06  0.000005  ...  0.004715  0.000005   \n",
      "9997  0.000003  3.004808e-06  3.004808e-06  0.057073  ...  0.000003  0.001437   \n",
      "9998  0.000006  5.820722e-06  5.820722e-06  0.004603  ...  0.000006  0.027965   \n",
      "9999  0.000526  1.347370e-01  5.263158e-04  0.000526  ...  0.000526  0.000526   \n",
      "\n",
      "              w_92      w_93      w_94          w_95          w_96      w_97  \\\n",
      "0     2.898551e-05  0.000029  0.000029  2.898551e-05  2.898551e-05  0.000029   \n",
      "1     4.659832e-06  0.000005  0.000005  4.659832e-06  4.659832e-06  0.000005   \n",
      "10    6.618134e-06  0.000007  0.000007  6.618134e-06  6.618134e-06  0.000007   \n",
      "100   6.193101e-07  0.019980  0.000284  6.193101e-07  6.193101e-07  0.006048   \n",
      "1000  3.003003e-05  0.010792  0.000030  5.034911e-03  3.003003e-05  0.000030   \n",
      "...            ...       ...       ...           ...           ...       ...   \n",
      "9995  4.975124e-05  0.000050  0.000050  4.975124e-05  4.975124e-05  0.000050   \n",
      "9996  5.356186e-06  0.000005  0.000005  5.356186e-06  1.647128e-02  0.000005   \n",
      "9997  3.004808e-06  0.000003  0.002256  3.004808e-06  3.004808e-06  0.000003   \n",
      "9998  5.820722e-06  0.000006  0.000006  5.820722e-06  5.820722e-06  0.033400   \n",
      "9999  5.263158e-04  0.000526  0.000526  5.263158e-04  5.263158e-04  0.000526   \n",
      "\n",
      "          w_98      w_99  \n",
      "0     0.000029  0.000029  \n",
      "1     0.000005  0.000005  \n",
      "10    0.000007  0.000007  \n",
      "100   0.000794  0.000675  \n",
      "1000  0.000030  0.000030  \n",
      "...        ...       ...  \n",
      "9995  0.000050  0.005096  \n",
      "9996  0.000005  0.000005  \n",
      "9997  0.001681  0.000003  \n",
      "9998  0.000006  0.000006  \n",
      "9999  0.000526  0.000526  \n",
      "\n",
      "[33226 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "print(node_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make graphs great again\n",
    "\n",
    "Time to get training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples shape: (453797, 3)\n",
      "Testing examples shape: (113450, 2)\n"
     ]
    }
   ],
   "source": [
    "# Read training\n",
    "with open(r\"training.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    training  = list(reader)\n",
    "# in order of training examples\n",
    "training = [element[0].split(\" \") for element in training]\n",
    "training = pd.DataFrame(training, columns=['Node1', 'Node2', 'Link'])\n",
    "print(\"Training examples shape: {}\".format(training.shape))\n",
    "\n",
    "# Read testing\n",
    "with open(r\"testing.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    testing  = list(reader)\n",
    "# in order of testing examples\n",
    "testing = [element[0].split(\" \") for element in testing]\n",
    "testing = pd.DataFrame(testing, columns=['Node1', 'Node2'])\n",
    "print(\"Testing examples shape: {}\".format(testing.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_nodes = training.loc[training['Link']=='1']\n",
    "linked_nodes = linked_nodes[['Node1', 'Node2']]\n",
    "linked_nodes.to_csv('linked_nodes.txt', sep=' ', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 1000 edges\n",
      "Removed 2000 edges\n",
      "Removed 3000 edges\n",
      "Removed 4000 edges\n",
      "Removed 5000 edges\n",
      "Removed 6000 edges\n",
      "Removed 7000 edges\n",
      "Removed 8000 edges\n",
      "Removed 9000 edges\n",
      "Removed 10000 edges\n",
      "Removed 11000 edges\n",
      "Removed 12000 edges\n",
      "Removed 13000 edges\n",
      "Removed 14000 edges\n",
      "Removed 15000 edges\n",
      "Removed 16000 edges\n",
      "Removed 17000 edges\n",
      "Removed 18000 edges\n",
      "Removed 19000 edges\n",
      "Removed 20000 edges\n",
      "Removed 21000 edges\n",
      "Removed 22000 edges\n",
      "Removed 23000 edges\n",
      "Removed 24000 edges\n",
      "Removed 25000 edges\n",
      "Removed 26000 edges\n",
      "Removed 27000 edges\n",
      "Removed 28000 edges\n",
      "Sampled 1000 negative examples\n",
      "Sampled 2000 negative examples\n",
      "Sampled 3000 negative examples\n",
      "Sampled 4000 negative examples\n",
      "Sampled 5000 negative examples\n",
      "Sampled 6000 negative examples\n",
      "Sampled 7000 negative examples\n",
      "Sampled 8000 negative examples\n",
      "Sampled 9000 negative examples\n",
      "Sampled 10000 negative examples\n",
      "Sampled 11000 negative examples\n",
      "Sampled 12000 negative examples\n",
      "Sampled 13000 negative examples\n",
      "Sampled 14000 negative examples\n",
      "Sampled 15000 negative examples\n",
      "Sampled 16000 negative examples\n",
      "Sampled 17000 negative examples\n",
      "Sampled 18000 negative examples\n",
      "Sampled 19000 negative examples\n",
      "Sampled 20000 negative examples\n",
      "Sampled 21000 negative examples\n",
      "Sampled 22000 negative examples\n",
      "Sampled 23000 negative examples\n",
      "Sampled 24000 negative examples\n",
      "Sampled 25000 negative examples\n",
      "Sampled 26000 negative examples\n",
      "Sampled 27000 negative examples\n",
      "Sampled 28000 negative examples\n",
      "** Sampled 28362 positive and 28362 negative edges. **\n",
      "Removed 1000 edges\n",
      "Removed 2000 edges\n",
      "Removed 3000 edges\n",
      "Removed 4000 edges\n",
      "Removed 5000 edges\n",
      "Removed 6000 edges\n",
      "Removed 7000 edges\n",
      "Removed 8000 edges\n",
      "Removed 9000 edges\n",
      "Removed 10000 edges\n",
      "Removed 11000 edges\n",
      "Removed 12000 edges\n",
      "Removed 13000 edges\n",
      "Removed 14000 edges\n",
      "Removed 15000 edges\n",
      "Removed 16000 edges\n",
      "Removed 17000 edges\n",
      "Removed 18000 edges\n",
      "Removed 19000 edges\n",
      "Removed 20000 edges\n",
      "Removed 21000 edges\n",
      "Removed 22000 edges\n",
      "Removed 23000 edges\n",
      "Removed 24000 edges\n",
      "Removed 25000 edges\n",
      "Sampled 1000 negative examples\n",
      "Sampled 2000 negative examples\n",
      "Sampled 3000 negative examples\n",
      "Sampled 4000 negative examples\n",
      "Sampled 5000 negative examples\n",
      "Sampled 6000 negative examples\n",
      "Sampled 7000 negative examples\n",
      "Sampled 8000 negative examples\n",
      "Sampled 9000 negative examples\n",
      "Sampled 10000 negative examples\n",
      "Sampled 11000 negative examples\n",
      "Sampled 12000 negative examples\n",
      "Sampled 13000 negative examples\n",
      "Sampled 14000 negative examples\n",
      "Sampled 15000 negative examples\n",
      "Sampled 16000 negative examples\n",
      "Sampled 17000 negative examples\n",
      "Sampled 18000 negative examples\n",
      "Sampled 19000 negative examples\n",
      "Sampled 20000 negative examples\n",
      "Sampled 21000 negative examples\n",
      "Sampled 22000 negative examples\n",
      "Sampled 23000 negative examples\n",
      "Sampled 24000 negative examples\n",
      "Sampled 25000 negative examples\n",
      "** Sampled 25526 positive and 25526 negative edges. **\n"
     ]
    }
   ],
   "source": [
    "# Read edges and create NetworkX graph\n",
    "edgelist = pd.read_csv(\"linked_nodes.txt\", sep=' ', header=None, names=[\"source\", \"target\"])\n",
    "edgelist[\"label\"] = \"cites\"  # set the edge type\n",
    "G_all_nx = nx.from_pandas_edgelist(edgelist, edge_attr=\"label\")\n",
    "nx.set_node_attributes(G_all_nx, \"paper\", \"label\")\n",
    "\n",
    "# Initialize Stellargraph with node features of text\n",
    "#G_all = sg.StellarGraph(G_all_nx, node_features=node_data[feature_names])\n",
    "\n",
    "# Define an edge splitter on the original graph G:\n",
    "edge_splitter_test = EdgeSplitter(G_all_nx)\n",
    "\n",
    "# Randomly sample a fraction p=0.1 of all positive links, \n",
    "# and same number of negative links, from G, and obtain the\n",
    "# reduced graph G_test with the sampled links removed:\n",
    "G_test, edge_ids_test, edge_labels_test = edge_splitter_test.train_test_split(\n",
    "    p=0.1, method=\"global\", keep_connected=True)\n",
    "\n",
    "# Define an edge splitter on the reduced graph G_test:\n",
    "edge_splitter_train = EdgeSplitter(G_test)\n",
    "\n",
    "# Randomly sample a fraction p=0.1 of all positive links, and same number of negative links, from G_test, and obtain the\n",
    "# reduced graph G_train with the sampled links removed:\n",
    "G_train, edge_ids_train, edge_labels_train = edge_splitter_train.train_test_split(\n",
    "    p=0.1, method=\"global\", keep_connected=True)\n",
    "\n",
    "G_test = sg.StellarGraph(G_test, node_features=node_data[feature_names])\n",
    "G_train = sg.StellarGraph(G_train, node_features=node_data[feature_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StellarGraph: Undirected multigraph\n",
      " Nodes: 33162, Edges: 255261\n",
      "\n",
      " Node types:\n",
      "  paper: [33162]\n",
      "    Edge types: paper-cites->paper\n",
      "\n",
      " Edge types:\n",
      "    paper-cites->paper: [255261]\n",
      "\n",
      "StellarGraph: Undirected multigraph\n",
      " Nodes: 33162, Edges: 229735\n",
      "\n",
      " Node types:\n",
      "  paper: [33162]\n",
      "    Edge types: paper-cites->paper\n",
      "\n",
      " Edge types:\n",
      "    paper-cites->paper: [229735]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(G_test.info())\n",
    "print(G_train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "epochs = 100\n",
    "num_samples = [20, 10]\n",
    "\n",
    "train_gen = GraphSAGELinkGenerator(G_train, batch_size, num_samples).flow(\n",
    "    edge_ids_train, edge_labels_train, shuffle=True)\n",
    "test_gen = GraphSAGELinkGenerator(G_test,  batch_size, num_samples).flow(\n",
    "    edge_ids_test, edge_labels_test)\n",
    "\n",
    "layer_sizes = [20, 20]\n",
    "assert len(layer_sizes) == len(num_samples)\n",
    "\n",
    "graphsage = GraphSAGE(\n",
    "        layer_sizes=layer_sizes, generator=train_gen, bias=True, dropout=0.3)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "link_classification: using 'ip' method to combine node embeddings into edge embeddings\n"
     ]
    }
   ],
   "source": [
    "# Build the model and expose input and output sockets of graphsage model for link prediction via graphsage.build() method\n",
    "x_inp, x_out = graphsage.build()\n",
    "\n",
    "prediction = link_classification(\n",
    "    output_dim=1, output_act=\"relu\", edge_embedding_method='ip')(x_out)\n",
    "model = keras.Model(inputs=x_inp, outputs=prediction)\n",
    "\n",
    "model.compile(\n",
    "        optimizer=keras.optimizers.Adam(lr=1e-2),\n",
    "        loss=keras.losses.binary_crossentropy,\n",
    "        metrics=[\"acc\"],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    }
   ],
   "source": [
    "#init_train_metrics = model.evaluate_generator(train_gen)\n",
    "#init_test_metrics = model.evaluate_generator(test_gen)\n",
    "#\n",
    "#print(\"\\nTrain Set Metrics of the initial (untrained) model:\")\n",
    "#for name, val in zip(model.metrics_names, init_train_metrics):\n",
    "#    print(\"\\t{}: {:0.4f}\".format(name, val))\n",
    "#\n",
    "#print(\"\\nTest Set Metrics of the initial (untrained) model:\")\n",
    "#for name, val in zip(model.metrics_names, init_test_metrics):\n",
    "#    print(\"\\t{}: {:0.4f}\".format(name, val))\n",
    "    \n",
    "history = model.fit_generator(\n",
    "    train_gen,\n",
    "    epochs=epochs,\n",
    "    validation_data=test_gen,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
